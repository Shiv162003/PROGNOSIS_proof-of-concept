company,role,salary,experience,Location,description,skills,qualification,industry_type,Functional_area,Employment_type,Role_category
"IHXs vision is to build a Data Exchange and, develop AI/ML infused healthcare solutions delivering superlative experience to every stakeholder in the healthcare sector across all segments of the market. As we undertake this exciting journey together, we will be driven by a higher purpose to make healthcare availability, affordability and predictability a reality and believe that the solutions we develop and/or enable will leave a lasting impact on our society and country.","RoleData Engineer,","₹ 30,00,000 - 45,00,000 P.A. ",10 - 15 years,Bangalore/Bengaluru,"Job descriptionRoles and Responsibilities      Own, design, build, operationalize, secure, and monitor data processing systems     Translating business use cases across all products into technical specifications, including data     streams, integrations, transformations, databases, and data warehouses     Defining the data architecture framework, standards and principles, including modelling, metadata,     security     Evaluate tools/software required for development of data platform by conducting proof-of-concepts     Build logical and physical data models in sustainable, scalable manner and Design various ETL      processes defining data flows across data stores/systems     Collaborate with Product Owner, IT, business process owners and data scientists to     design/build/improve Data Warehouse and data marts     Be a Tech Anchor for the team on streaming & batch ETL development activities conducting code     reviews, and resolving technical issues and dependencies     Optimizing queries and building a scalable data model     Monitor processes and develop plans to capture and access all metadata, according to various ETL     processes.     Evaluate all proposals requests and improve structure of data warehouse     Ensure accuracy and quality of data movement across ETL pipelines in production environmentDesired Candidate Profile An SME with over 10+ years of deep expertise in building and understanding Data driven high availability applications that can be deployed to modern day vendor agnostic cloud infrastructures Proven experience in requirement gathering, Data curation and testing high performanceand distributed scalable computing solutions Hands on expertise in building and implementing data architecture for large enterprises. Real world experience of building scalable products that can withstand data volume of internet scale using technologies/frameworks such as Kafka, Spark, Kubernetes, Serverless and other parallel processing open-source and managed frameworks on large data volumes or data sets Well versed with Agile methodology for managing project deliverables. Someone who has traversed the data journey, having moved the clients through the modernization of data warehouses to data lakes to data operations on the cloud. Expertise in cloud-based data warehouse / data lake implementation (AWS/Azure) Hands-on experience in ETL, data warehouse, data lake, business intelligence, data governance, MDM, data virtualization and data ops technologies. Sound understanding of Relational, Graph and NoSQL data stores Proficient in testing Data pipeline and Analytics pipeline. Executed two or more engagements covering end to end lifecycle of nextgen data project (framing data strategy, define & drive new age data architecture, its transition to cloud and subsequent adoption) Consulted clients on aspiration to make them information led decision making business. Experience in deploying applications to a Cloud platform ( AWS/Azure) Ability to proactive pick up cutting edge trends in the industry Perks and Benefits Best In the IndustryRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Any SpecializationKey Skillsdata architectureAzure Data FactoryAzure DatabricksSkills highlighted with ‘‘ are preferred keyskills","['data architecture', 'Azure Data Factory', 'Azure Databricks']",['UG :B.Tech/B.E. in Any Specialization'],"Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 674,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at accenture.com","RoleTechnical Architect,",Not Disclosed,12 - 15 years,Bangalore/Bengaluru,"Job description  Project Role :  Data Architect    Project Role Description :  Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.    Management Level :  7    Work Experience :  12-15 years    Work location :  Bengaluru    Job Requirements :        Key Responsibilities :   1Function as the Lead Data Architect for a small, simple project/proposal or as a team lead for medium/large sized project or proposal2Discuss specific data architecture and data related issues with client architect/team in area of expertise3Analyze and assess the impact of the requirements on the data and its lifecycle4Lead the data architecture and design of complex, enterprise-level applications and systems5Breadth of experience in various client scenarios and situations     Technical Experience :   1Strong experience in Azure is preferred with hands-on experience in 2 or more of these skills : Azure SQL DB ,Azure SQL Managed Instance ,Azure Data Lake Store, Azure Cosmos DB, Azure Database for PostgreSQL, Azure Database for MySQL2Experience in handling medium to large data migration projects3 5 years of extensive database experience design build     Professional Attributes :   1Should be able to drive the technology design meetings, propose technology design and architecture 2Should have excellent client communication skills3Should have good analytical and problem-solving skills     Educational Qualification :   1Must have: BE/BTech/MCA 2Good to have: ME/MTech  RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :M.Tech, MCA in ComputersKey SkillsData migrationPostgresqlMySQLConsultingSQLArchitectureAnalyticalCosmosData architectureSkills highlighted with ‘‘ are preferred keyskills","['Data migration', 'Postgresql', 'MySQL', 'Consulting', 'SQL', 'Architecture', 'Analytical', 'Cosmos', 'Data architecture']","['UG :B.Tech/B.E.', 'PG :M.Tech, MCA in Computers']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"We Are Brillio. Born of the digital age in 2014. We are laser-focused on turning technological disruptions into the advantages that our customers need to thrive in todays digital economy.Brilliois a company focused on digital technologies andbig dataanalytics headquartered inSanta Clara, California, United States","RoleDatabase Architect / Designer,",Not Disclosed,6 - 10 years,Bangalore/Bengaluru,"Job description  BSc or MSc (preferred) in a STEM field Relevant work experience of 5 years Fluency in Python (especially Numpy and Pandas) and familiarity in PySpark Extensive hands-on experience with AWS Analytical Components like S3, EC2, Lambdas, Glue, SQS, SNS, DynamoDB, Redshift, RDS etc.    Experience with Data Lake Formation and Athena    Work Experience with industry standard distributed systems (ie    Spark, hive), data pipeline tools (ie    Airflow), NoSQL Databases (DynamoDB) and databases (PostgreSQL) Experience with Data Analysis, Significant experience optimizing data retrieval processes supporting API output, ideally within a low query volume / high data volume environment    Demonstrably deep experience with relevant big data processing either via Spark or through a modern MPP database like Redshift, ideally with experience in both Demonstrably deep experience with CI/CD tools and practices in a containerized AWS environment, from deployment pipelines (Jenkins, etc), infrastructure definition (Terraform, CloudFormation, etc    Understand and design for non-functional concerns such as performance, cost optimization, maintainability, and developer experience.             RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData analysisNoSQLAnalyticalPostgresqlData ArchitectData processingbig dataDistribution systemCost optimizationPython","['Data analysis', 'NoSQL', 'Analytical', 'Postgresql', 'Data Architect', 'Data processing', 'big data', 'Distribution system', 'Cost optimization', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"A Silicon-Valley headquartered company, Infogain is a global business oriented IT consulting provider of front-end, customer-facing technologies, processes and applications, leading to a more efficient and streamlined customer experience. We want our clients€™ interactions with their customers to be fast, efficient, and cost effective.With close to 4,000 employees in the United States, India, the Middle East, U.K., Singapore and Malaysia, we service 5 of the world€™s largest 50 companies, and 24 of the Fortune 500. we have million-dollar engagements with over 25 customers, many of which have been with us for 5 years or more.At Infogain, we place a high value on establishing long-term relationships with our clients, ultimately becoming virtual extensions of their organizations. In fact, more than 90% of our medium and large deal size clients from five years ago remain clients today. Why? Our consultants, project managers and engineering teams listen and address our clients€™ specific requirements with best-in-class solutions across a broad spectrum of service areas.Infogain is an Oracle Knowledge Management Expert, having the world€™s largest practice to improve customer support operations and boost satisfaction in High Tech, Insurance, Travel & Hospitality and Retail.We are also a global leader in Oracle Retail€™s customer facing products.For insurance companies, Infogain helps make the claims process more efficient, effective and customer friendly.","RoleData Engineer,",Not Disclosed,8 - 11 years,Noida,"Job descriptionGreeting form Infogain! We are having Immediate requirement for Azure Data Architect in Infogain India Pvt Ltd. Please Find the Job Description below & If you are interested please share your updated Resume with details:-Mode of Hiring-PermanentExperince-8-12 YrsSkills Required- ADF, SQL, MS SQL Server, SynapseNotice Period- Immediate to 30 Days MaxLocation- Noida/Bangalore/Pune/Mumbai (Currently work from home)Job Description 6+ Years experience in Azure Stack(ADF, ADLS, Synapse, Databricks, PowerBI) and strong Data Warehouse, SQL DB, Data Modelling Skills. Hands-on exposure to coding and mentor team on technical abilities Kindly share your update word formatted /pdf profile with the details below on arti.sharma@infogain.com Total exp-Exp in ADFExp in SynapseExp in DatabricksCurrent CTC:Exp CTC:NP -Current Location-Preferred Location- About Infogain:Infogain is a Silicon Valley headquartered company with expertise in software platform engineering and deep domain skills in travel, retail, insurance, automotive, and high technology. We accelerate the delivery of digital customer engagement systems using digital technologies such as cloud, microservices, robotic process automation, and artificial intelligence for our clients. Our unique engagement approach of Listen-Curate-Deliver helps to accelerate the innovation journey of 5 of the worlds largest 50 companies and 24 of the Fortune 500, with several relationships of over 10 years. We deliver positive business outcomes using rapid prototyping and a solid foundation of DevOps-based software platform engineering that ensure high-quality and on-time delivery. Our 3,500 global employees across the US, UK, Singapore, Middle East and India focus on client value creation, delivery excellence and innovation. Our locations in India have 2700 employees spanning Noida, Pune, Mumbai and Bangalore.Infogain maintains both strategic and technology partnerships with leading enterprise software providers to deliver value-added solutions. We engage with the world's largest, as well as mid-size, and startup, software providers for building product capability, product marketing, customization, professional services and post-implementation supportRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Any DoctorateKey SkillsAzure Data FactoryADFSkills highlighted with ‘‘ are preferred keyskills","['Azure Data Factory', 'ADF']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Any Doctorate']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 674,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at accenture.com","RoleTechnical Architect,",Not Disclosed,6 - 8 years,Bangalore/Bengaluru,"Job description  Project Role :  Data Architect    Project Role Description :  Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.    Management Level :  9    Work Experience :  6-8 years    Work location :  Bengaluru    Job Requirements :        Key Responsibilities :   aFunction as the Lead Data Architect for a small, simple project/proposal or as a team lead for medium/large sized project or proposalbDiscuss specific Big data architecture and related issues with client architect/team in area of expertisecAnalyze and assess the impact of the requirements on the data and its lifecycledLead Big data architecture and design medium-big Cloud based, Big Data and Analytical Solutions using Lambda architectureeBreadth of experience in various client scenario     Technical Experience :   aStrong experience in Azure is preferred with hands-on experience in two or more of these skills : Azure Synapse Analytics, Azure HDInsight, Azure Databricks with PySpark / Scala / SparkSQL, Azure Analysis ServicesbExperience in one or more Real-time/Streaming technologies including: Azure Stream Analytics, Azure Data Explorer, Azure Time Series Insights, etccExperience in handling medium to large Big Data implementationsdFor Level 8 - Candidate must have 10-12 years of IT experience     Professional Attributes :   aShould be able to drive the technology design meetings, propose technology design and architecture bShould have excellent client communication skillscShould have good analytical and problem-solving skills     Educational Qualification :   aMust have: BE/Btech/MCA bGood to have: ME/Mtech  RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :MCA in Computers, M.TechKey SkillsAnalyticalConsultingSCALAAnalyticsData architectureArchitecturebig dataSkills highlighted with ‘‘ are preferred keyskills","['Analytical', 'Consulting', 'SCALA', 'Analytics', 'Data architecture', 'Architecture', 'big data']","['UG :B.Tech/B.E.', 'PG :MCA in Computers, M.Tech']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Triangle ( A Unit of Ratein Infotech India Private Ltd) is a Recruitment Specialist Organisation with focus on Information Technology Industry. Incorporated in August 2000 with 4 offices in India at Mumbai, Pune, New Delhi & Bangalore, Triangle caters to more than 25 active clients who are Fortune 500 organizations and world¢??s largest IT organizations.  Our Core expertise is in End to End Permanent and Contract Staffing and Extended Services.  We have been Internationally recognized as WeConnect Certified woman owned and managed organization.  Triangle has featured in CIO Review Magazine as ¢??Company of the Year in Staffing and Recruitment¢? Category for year 2014.","RoleSolution Architect,",Not Disclosed,10 - 15 years,Bangalore/Bengaluru,"Job description   Experience in architecting with AWS or Azure Cloud Data Platform Successfully implemented large-scale data warehouse / data lake solutions in snowflake or AWS Redshift Be proficient in Data modelling and data architecture design experienced in reviewing 3rd Normal Form and Dimensional models    Experience in implementing Master data management, process design and implementation    Experience in implementing Data quality solutions including processes    Experience in IOT Design using AWS or Azure Cloud platforms    Experience designing and implementing machine learning solutions as part of high-volume data ingestion and transformation    Experience working with structured and unstructured data including geo-spatial data    Experience in technologies like python, SQL, no SQL, KAFKA, Elastic Search Hands on experience using snowflake, informatica, azure logic apps, azure functions, azure storage, azure data lake and azure search    Behaviors Required: Driven by our values and purpose in everything we do Visible, active, hands on approach to help teams be successful Strong proactive planning ability    Optimistic, energetic, problem solver, ability to see long term business outcomes    Collaborative, ability to listen, compromise to make progress    Stronger together mindset, with a focus on innovation & creation of tangible / realized value      Education      Qualifications, Accreditation, Training: Required: Degree in Computer Science and/or related fields Azure or AWS Solution Architecture certifications    RoleSolution Architect,Industry TypeRecruitment / Staffing,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsComputer scienceSolution architectureProcess designMachine learningData ArchitectData qualityInformaticaSQLPythonData architecture","['Computer science', 'Solution architecture', 'Process design', 'Machine learning', 'Data Architect', 'Data quality', 'Informatica', 'SQL', 'Python', 'Data architecture']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeRecruitment / Staffing,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Eli Lilly and Company strives to create informative and engaging online communities that share information in meaningful ways. We welcome and encourage your participation and engagement in thoughtful and respectful discourse.,"RoleTechnical Architect,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job description       Deep technical understanding of various data architectures and solution options along with an understanding of business requirements.    Deep understanding of cloud technologies and strategies including knowledge on cloud infrastructure (Azure/ AWS)    Ability to create roadmaps through working with multiple stakeholders Global Technology Teams and Business    Translating requirements into technical design and architecture, aligned with enterprise and industry standards and best practices, along with the corresponding required documentation.    Well-versed in both the evolving technology landscape as well as the needs of the business areas they support.    Has a deep understanding of data sources, flows, integration, acquisition, structures, quality, harmonization, and governance.    Translate business requirements into consumable data models and source to target data mapping.    Consult with internal customers who own the data to develop information relationships that lead to actionable insights.    Work with data from multiple data sources to build integrated views that will drive decisions.    Work in Agile Sprints with business facing project teams.      Additional Responsibilities        Collaborates with architects on technical design issues, particularly for solutions that impact local and global business areas    Work with Global Engineering Business Partners and Sites to understand business problems and set IT direction roadmap for solutions to meet business needs    Work with central and site engineering business partners and define processes for capture and maintaining engineering data, such as equipment metadata, supporting to the goal of creating an equipment digital twin for key process equipment sets and integration of this metadata with various sources of data.    Perform data issue analysis and work with various MQ teams to improve data quality confidence.    Serve as the technical liaison between the Bigdata Engineering admins, Data Scientists, Data Engineers and Product Owners.    Validate data quality coverage and accuracy by developing reports and tools to monitor and visualize data quality    Support the effort in finding data quality gaps by working with business and across IT domains to identify process or data management changes.    Work with Data Integration Developers to build solutions.    Work closely with the Data Integration and Analytics team to ensure we are in sync with the NextGen data lake    Develop deep understanding of technologies that enable business processes within key areas.    Develop solid understanding of business processes and needs to support recommended technical direction.    Responsible for ensuring adherence to technology roadmaps driven by the Lilly Enterprise Architecture community.    Drives decisions with respect to technologies and use of those technologies.    Ensure interface and data needs are understood and that the necessary technology and architecture are in place to meet these needs.    Responsible for understanding, influencing, and evolving the supported application(s) technical design.    Understanding of current technology as well as future direction for the area and the enterprise.        Basic Requirements      BachelorDegree and 5 yearsexperience in the implementation of modern data ecosystems.    Expertise in Python, SQL, Hadoop, Hive, Spark, UNIX Shell scripting, and Informatica big data technologies.    Expertise with JSON, REST API, and other data integration technologies.    Experience in one or more cloud-based data solutions/ cloud infrastructure    Demonstrated understanding of data privacy and CCI requirements and experience delivering within those requirements.    Experience in applying quality and compliance requirements.    Excellent oral and written communication skills.    Fluency in English.    A high level of intellectual curiosity, external perspective, and innovation interest    Strong analytical, problem solving and investigative skills    Experience with security models and development on large data sets    Excellent self-management skills.    RoleTechnical Architect,Industry TypePharmaceutical & Life Sciences,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsComputer scienceData managementAgileHealthcareJSONInformaticaUnix shell scriptingAnalyticsSQLPython","['Computer science', 'Data management', 'Agile', 'Healthcare', 'JSON', 'Informatica', 'Unix shell scripting', 'Analytics', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePharmaceutical & Life Sciences,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Eli Lilly and Company strives to create informative and engaging online communities that share information in meaningful ways. We welcome and encourage your participation and engagement in thoughtful and respectful discourse.,"RoleDatabase Architect / Designer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job description       Minimum 5 years Pharmaceutical Manufacturing experience.Knowledge of GxP, Supply Chain, Pharmaceutical manufacturing processes and automations systems    Minimum 5 years of relevant experience with data architecture design    Experience in Computer System Validation processes and Data Integrity concepts    Excellent communication, interpersonal, and influencing skills.    Ability to carry on conversations with business customers, upper management, and technical personnel.    Experience in working in an international and multicultural environment where team members are spread into different countries    Ability to thrive in a complex, changing, virtual environment with multiple competing priorities    Strong problem solving and organizational skills    A high level of intellectual curiosity, external perspective, and innovation interest    Strong analytical, problem solving and investigative skills    Experience with security models and development on large data sets    Demonstrated understanding of data privacy and CCI requirements and experience delivering within those requirements.    Excellent self-management skills.          Technology Background:      Experience with Agile Methodology    ITIL certification    Knowledge of best practices for code review and management in GitHub    Experience in design and development of data lakes, data warehouses, data marts and schema design    Building and maintaining complex data pipelines    Expertise in performing data extraction, profiling, cleansing, conversion, transformation and loading of data    Knowledge of CI/CD, Infrastructure as Code and more generally Everything as Code    Experience with DBMS software and tools (PostgreSQL, Oracle, Snowflake, Teradata, MS SQL Server, MongoDB, Toad, Hadoop, CouchDB, etc.)    Expertise in data management (structured/unstructured), data mining and reporting technologies    Extensive experience with data modeling using Erwin, PowerDesigner, Toad or similar software    Experience working with Amazon AWS, Microsoft Azure, Spark framework    Experience with a multitude of AWS services; especially Glue, DMS, CloudFormation, IAM, Route53, ALB/ELB, VPC, EC2, Lambda, S3, KMS, CloudTrail, Config, CloudWatch    Experience with Azure services; especially Azure Data Factory, Azure Resource Manager, Azure Synapse, Azure Functions, Azure Data Lake Analytics, Azure Data Lake Storage, Azure Monitor    Experience with SQL, PL/SQL, Python, Spark Framework, YAML, JSON, Chef    Experience with Docker, NodeJS, Ansible, Docker, Jenkins, Unix, Linux    Strong understanding of cybersecurity operation principles    Knowledge of industry standard NIST framework        Additional Preferences:      Knowledge of data science technologies like R, Keras or TensorFlow    Knowledge of ML/AI and natural language processing    Experience implementing and leading data governance    Experience working in a regulated environment    Manufacturing Operations experience preferred    Pharma Manufacturing experience strongly desired.    Multi-Site, Global IT Project Experiences strongly desired    RoleDatabase Architect / Designer,Industry TypePharmaceutical & Life Sciences,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateKey SkillsUnixMS SQLLinuxData modelingData ArchitectPLSQLJSONOracleTeradataPython","['Unix', 'MS SQL', 'Linux', 'Data modeling', 'Data Architect', 'PLSQL', 'JSON', 'Oracle', 'Teradata', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePharmaceutical & Life Sciences,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"ICON CLINICAL RESEARCH INDIA PRIVATE LIMITED is located in Chennai, Tamil Nadu, India and is part of the Scientific Research and Development Services Industry. ICON CLINICAL RESEARCH INDIA PRIVATE LIMITED has 2,444 total employees across all of its locations. There are 10 companies in the ICON CLINICAL RESEARCH INDIA PRIVATE LIMITED corporate family.","RoleTechnical Architect,",Not Disclosed,5 - 9 years,Bangalore/Bengaluru,"Job description  As a    Data Architect II   you will be responsible for helping to maintain and optimize the data analysis infrastructure that is used to drive the processes necessary to allow the Medical Informatics team to provide Real World Evidence solutions that support various critical aspects of clinical research to ICON s internal and external stakeholders.      Responsibilities     Efficiently move large volumes of data from diverse data platforms into Hadoop platform   Work with large volumes of data so as to derive Business Intelligence   Write scalable and maintainable ETLs   Managing Hadoop jobs using scheduler   Write solid, adaptable, and high-performing code   Strong understanding of Impala, Hive, HBase, Pig, and Sqoop   Good understanding of Linux script          What you need:      Degree qualified - Computer Science or other STEM discipline     Healthcare RWD experience is essential for this role. 3+ years experience working with transactional health data such as medical, prescription, and EMR/EHR is required.     Relational database and/or DFS experience, specifically: 5+ years of experience in Hadoop (Impala/Hive)   Programming skills, specifically: 3+ years of experience in SQL/HPLSQL ; 2+ years of experience in Python, Java, C#   Platform flexibility specifically: 3+ years of experience in Linux (SSH/SFTP)   RoleTechnical Architect,Industry TypePharmaceutical & Life Sciences,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsDFSData analysisLinuxPharmaData ArchitectClinical researchHealthcarePublic healthSQLPython","['DFS', 'Data analysis', 'Linux', 'Pharma', 'Data Architect', 'Clinical research', 'Healthcare', 'Public health', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePharmaceutical & Life Sciences,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Triangle ( A Unit of Ratein Infotech India Private Ltd) is a Recruitment Specialist Organisation with focus on Information Technology Industry. Incorporated in August 2000 with 4 offices in India at Mumbai, Pune, New Delhi & Bangalore, Triangle caters to more than 25 active clients who are Fortune 500 organizations and world¢??s largest IT organizations.  Our Core expertise is in End to End Permanent and Contract Staffing and Extended Services.  We have been Internationally recognized as WeConnect Certified woman owned and managed organization.  Triangle has featured in CIO Review Magazine as ¢??Company of the Year in Staffing and Recruitment¢? Category for year 2014.","RoleTechnical Architect,",Not Disclosed,10 - 15 years,Bangalore/Bengaluru,"Job description    Azure Data Architect with streaming experience Total experience in data management area for 10 + years with Azure cloud data platform experience        Architect with Azure stack (ADLS, AALS, Azure Data Bricks, Azure Streaming        Analytics Azure Data Factory, cosmos DB Azure synapse) mandatory expertise on Azure streaming Analytics ,Data Bricks, Azure synapse, Azure cosmos DB        Must have worked experience in large Azure Data platform and dealt with high volume Azure streaming Analytics        Experience in designing cloud data platform architecture, designing large scale environments        5 plus Years of experience architecting and building Cloud Data Lake, specifically Azure Data Analytics technologies and architecture is desired,      Enterprise Analytics Solutions. ‚Leading development of Data Lake Architectures from scratch for streaming Analytics Platform ‚5+ years of Programming experience in Python, SQL, Spark ‚Experience on Azure data explorer (ADX) is good to have it       RoleTechnical Architect,Industry TypeRecruitment / Staffing,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData managementArchitectural designData ArchitectCloudProgrammingData analyticsArchitectingCosmosSQLPython","['Data management', 'Architectural design', 'Data Architect', 'Cloud', 'Programming', 'Data analytics', 'Architecting', 'Cosmos', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeRecruitment / Staffing,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
" Thanks & Best Regards,Hemalatha| Lead Talent Acquisition  www.cygnuspro.comCygnusPro Software Solutions Pvt. Ltd.Tel:     04068196805Mob:  9989759380email:  hemalatha@cygnuspro.com","RoleData warehouse Architect / Consultant,",Not Disclosed,15 - 20 years,Bangalore/Bengaluru,"Job description  Exp  15 + Yrs ,  Expertise in IICS Data Integration & App Integration                              Good Data Solutioning  Experience                              Exposure to Snowflake / Data Bricks                              Exposure to Java / Python will be added advantage                                Strong stakeholder management & Communication skills                              Exposure to Informatica EDC / Axon & MDMRoles and Responsibilities Desired Candidate Profile   Exp  15 + Yrs ,  Expertise in IICS Data Integration & App Integration                              Good Data Solutioning  Experience                              Exposure to Snowflake / Data Bricks                              Exposure to Java / Python will be added advantage                                Strong stakeholder management & Communication skills                              Exposure to Informatica EDC / Axon & MDMPerks and Benefits RoleData warehouse Architect / Consultant,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, Temporary/ContractualRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData SolutioningIICS Data Integration and App IntegrationInformatica EDC / Axon & MDMJava / PythonSnowflake / Data BricksSkills highlighted with ‘‘ are preferred keyskills","['Data Solutioning', 'IICS Data Integration and App Integration', 'Informatica EDC / Axon & MDM', 'Java / Python', 'Snowflake / Data Bricks']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Temporary/Contractual",Role CategoryDBA / Data warehousing
"Established in the year 2017, with a view to providing Development and Staffing Solutions. We are driven by the goal to provide services that change lives. We are evolving as the manifestation of our founders, where we resolve complex development and staffing issues of our clients with strong ethical and proficient foundation.","RoleDatabase Architect / Designer,",Not Disclosed,7 - 10 years,Bangalore/Bengaluru,"Job description You should have expertise in designing, implementing, and operating stable, scalable, solutions to flow data from production systems into analytical data platform (big data tech stack + MPP) and into end-user facing applications for both real-time and batch use cases.  The ability to do deep problem solving and build elegant, maintainable solutions to complex problems. Designing platforms as consumable data services across the organization using Big Data tech stack. Do high level design independently; Functional modelling, break-down of a module. Influence product requirements & operational plans. Instil best practices for development and champion their adoption, while working with product managers to estimate and plan projects in agile development framework.  Architectural & Design Choices, Deep knowledge on one or more tech stacks, identify alternative tech choices and trade-offs. Build and execute data modeling projects across multiple tech stacks i.e. big data, MPP, OLAP using agile development techniques. Strong problem Solving skills, Identify feasible alternatives and freeze on the optimal choice of design approach. Strong engineering mind set - build automated monitoring, alerting, self-healing (restart ability/graceful failures) features while building the consumption pipelines.   Challenge status quo and propose innovative ways to process, model, consume data when it comes to tech stack choices or design principles. Translate business requirements into technical specification (fact/dimension/filters/derivations/aggregations).   Lead by example, mentor and guide team members on everything from structured problem solving to development of best practices.  An ideal candidate will have excellent communication skills to be able to work with engineering, product and business owners to develop and define key business questions and to build data sets that answer those questions. you should bring your passion for working with huge data sets and bringing datasets together to answer business questions and drive change.RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :B.Tech/B.E. in Computers, BCA in Computers, B.Sc in ComputersPG :Any PostgraduateDoctorate :Doctorate Not RequiredKey Skillsbig dataHiveHadoopOLAPSparkMPPMap reduceScala programmingcommunication skillsSkills highlighted with ‘‘ are preferred keyskills","['big data', 'Hive', 'Hadoop', 'OLAP', 'Spark', 'MPP', 'Map reduce', 'Scala programming', 'communication skills']","['UG :B.Tech/B.E. in Computers, BCA in Computers, B.Sc in Computers', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"Our Company is a guaranteed for its exceptional ELDF ( Enterprise Level Solution Development Firm ) for giving the particular services to different businesses with numerous physio-graphic puts over the world. We make the association for the intellectual business by working with up and coming latest advancements to serve the best client experience through customized benefits and upgraded collaboration.We are having a range of products like Customized ERP, Smart Dashboard, Audit, CRM and IOT.","RoleTechnical Architect,",Not Disclosed,8 - 15 years,Kolkata,"Job description     Good Understanding of Distributed Data Platforms.    Should have worked as data architect in Implementing a medium/large scale Data Warehouse solution.    Experience in Migrating Legacy Data Warehousing Solution to GCP Cloud.    Deep exposure hands-on GCP Cloud Native ETL / ELT services with deep understanding of BigQuery and Looker or any other reporting platform.    Possess in depth knowledge and hands on development experience operationalizing large scale ingestion, processing, consumption using either DataProc or Dataflow or cloud fusion.    Strong understanding and experience with Storage infrastructure, event-based architecture using Cloud Functions, Monitoring, Logging, Auditing services of GCP.    Strong experience on either one or more MPP Data Warehouse Platforms prefer BigQuery, CloudSQL, Cloud Spanner, Fire store or similar.    Strong Development Experience on at least one or more event-driven streaming platforms prefer PUB/SUB, Kafka   Exposure to Networking on GCP and Gateway connectivity.    Strong Data Orchestration experience using tools such has Cloud Functions, Dataflow, Cloud Composer, Apache Airflow or related.   RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsSUBorchestrationNetworkingGCPCloudData Architectcloud storageApacheData warehousingMonitoring","['SUB', 'orchestration', 'Networking', 'GCP', 'Cloud', 'Data Architect', 'cloud storage', 'Apache', 'Data warehousing', 'Monitoring']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Triangle ( A Unit of Ratein Infotech India Private Ltd) is a Recruitment Specialist Organisation with focus on Information Technology Industry. Incorporated in August 2000 with 4 offices in India at Mumbai, Pune, New Delhi & Bangalore, Triangle caters to more than 25 active clients who are Fortune 500 organizations and world¢??s largest IT organizations.  Our Core expertise is in End to End Permanent and Contract Staffing and Extended Services.  We have been Internationally recognized as WeConnect Certified woman owned and managed organization.  Triangle has featured in CIO Review Magazine as ¢??Company of the Year in Staffing and Recruitment¢? Category for year 2014.","RoleTechnical Architect,",Not Disclosed,10 - 15 years,Bangalore/Bengaluru,"Job description    Azure Data Architect with streaming experience Job description Total experience in data management area for 10 + years with Azure cloud data platform experience Architect with Azure stack (ADLS, AALS, Azure Data Bricks, Azure Streaming Analytics Azure Data Factory, cosmos DB Azure synapse) mandatory expertise on Azure streaming Analytics ,Data Bricks, Azure synapse, Azure cosmos DB Must have worked experience in large Azure Data platform and dealt with high volume Azure streaming Analytics Experience in designing cloud data platform architecture, designing large scale environments 5 plus Years of experience architecting and building Cloud Data Lake, specifically Azure Data Analytics technologies and architecture is desired, Enterprise Analytics Solutions, and optimizing real time big data data pipelines, architectures and data sets     Job Description Architecting Microsoft Azure Solutions across multiple platforms    Implementation and Delivery of Microsoft Azure projects    Documentation of solutions (eg architecture, configuration and setup)    Working within a project management/agile delivery methodology in a leading role as part of a wider team    Experience of setting up, deploying and managing multiple environments to support agile development approaches hands-on experience with DevOps toolset    Knowledge of PowerShell, Git, ARM templates and deployment automation 10+ years of industry experience   Designing / Developing / Envisioning Enterprise apps using Azure IaaS and Analytics services with a focus on Azure RBAC, Azure AD, Azure Data Factory, Azure Data Lake Storage, Azure Function App, Azure Data Warehouse, Azure SQL DB, Azure Databricks      RoleTechnical Architect,Industry TypeRecruitment / Staffing,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsAutomationGITData managementPowershellProject managementAgile developmentData ArchitectArchitectural designCosmosSQL","['Automation', 'GIT', 'Data management', 'Powershell', 'Project management', 'Agile development', 'Data Architect', 'Architectural design', 'Cosmos', 'SQL']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeRecruitment / Staffing,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleTechnical Architect,",Not Disclosed,11 - 14 years,Bangalore/Bengaluru,"Job descriptionJob Description : Big Data Architect Function : Data Integration Location : Off Shore / India Basic Function : - The Big Data Architect works closely with the customer and the solutions architect to translate the customer's business requirements into a Big Data solution. - This includes understanding the customer data requirements, platform selection, design of the technical architecture, design of the application and interfaces, and development, testing, and deployment of the proposed solution. - Has the ability to design enterprise grade large-scale data processing systems and help identify the best options for architecture. - The Big Data Architect also understands the complexity of data and can design systems and models to handle different variety of data with varying levels of volume, velocity and veracity. - Should have independently worked on proposing architecture, design and data ingestion concepts in a consultative mode. - Leads client assessments, preparing current state and future state architectures along with go forward recommendations. - Will work with the practice leads and account management team to develop statements of work, implementation plans, resource plans and project estimates Essential Functions : - Has a deep understanding and experience with several of the following: Business Analysis, Requirements Gathering, Data Analysis, Data Modeling, Project Management, Project Estimation - Advanced knowledge of design and architecture patterns and methodologies. - Demonstrated work ethic, focus and self-discipline - Has and maintains a deep understanding of the role of big data in business and the enterprise. - Propose recommended and/or best practices regarding the movement, manipulation, and storage of data in a big data solution including data ingestion, data storage options, query techniques, data variety, volume & velocity, - Collaborate with project teams on platform development process from inception to production deployment, including project scheduling, design, implementation and coordination with other team members. - Collaborate with other technology teams and architects to define and develop cross- function technology stack interactions. - Research and experiment with emerging technologies and tools related to big data. - Experience in scaling applications on big data platforms to massive size. - Performing solution architecture in adherence to enterprise architecture governance. - Bridging business and development team. - Long term development and Technical expertise in DW/BI Practice, communicate well with all stakeholders, optimize objectives, leverage state of the art tools and best practices, integrate into corporate systems and deliver on time. - Deep understanding in Data Warehousing, Enterprise Architectures, Dimensional Modelling, Star & Snow-flake schema design, Reference DW Architectures, ETL Architect, ETL (Extract/Transform/Load), Data Analysis, Data Conversion/Transformation, Database Design, Data Warehouse Optimization, Data Mart Development, and Enterprise Data Warehouse Maintenance and Support etc. - Should have independently worked on proposing architecture, design and data ingestion concepts. Primary Internal Interactions : Project Delivery : - Formulates workable solutions that integrate people, process and technology. Integrates competency leads and other subject matter experts to define the details of vision and solution - Identifies, creates and communicates the vision of the end-to-end solution for the client and implementation teams after completing thorough evaluations of the present state needs - Leads technical teams in documenting and implementing technical solutions for business problems - Supports Delivery Enablement and Practice Leads by ensuring appropriate linkage between practices is clear and well understood - Works with practice leads to develop project estimating methodologies for new areas of focus Account & Resource Management : - Partners with the account teams to identify and sell solutions and create high level of client interest in proposed comprehensive solutions - Works closely with Engagement Managers, Account Managers and Account Leads in strategies to grow existing accounts - Partners with account team and client to develop the ROI, Business Case, and Statement of Work for an engagement - Develops comprehensive project, implementation, and resource plans working closely with Engagement and Project Managers Primary External Interactions : Assessment Participation & Leadership : - Identifies, creates and communicates the vision of the end-to-end solution for the client and implementation teams after completing thorough evaluations of the present state needs - Participates in strategy development and current state assessments for clients to identify opportunities. Acts as proxy for practice leads when necessary - Works with client to sell the proposed solution and develop internal marketing programs - Supports work that may not fall into an existing competency and helps to identify possible needs for new competencies. - Partners with account team and client to develop the ROI, Business Case, and Statement of Work for an engagement Organizational Relationships : Reports To : EDM India Practice Lead - over time, this will evolve Supervises : NA Skills : Technical Skills (Knowledge, Skills & Abilities) : - Experience with enterprise data management, Business Intelligence, data integration, and SQL database implementations - Experience with the major big data solutions like Hadoop, MapReduce, Hive, Spark, Scala, HBase, MongoDB, Cassandra. - Programming/scripting languages like Java, Linux, PHP, Ruby, Python and/or R. As well as have experience in working with ETL tools such as Informatica, Talend, Pentaho etc. - He or she should have experience in designing solutions for multiple large data warehouses with a good understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS and/or knowledge on NoSQL platforms. - Experience in data migration from relational databases to Hadoop HDFS - Propose best practices/standards - Translate, load and present disparate datasets in multiple formats/sources including JSON, XML etc. Must Have Skills : - Hadoop stack including HDFS cluster, MapReduce, Hive, Spark and Impala - Web Technologies CSS, DHTML, XML, Hight Charts, Linux - ETL tools such as Informatica, Talend and/or Pentaho. Query : SQL, No SQL Concepts Ingest : Kafka, Sqoop, Flume Orchestration : Zookeeper Databases : Postgres, Mongo DB, Cassandra, HBase Languages : Java, Scala Scripting : JavaScript, DHTML, XML, Shell Good to have Skills : Core : AWS, Hadoop, Yarn Process : Agile-Scrum, Iterative Development, DevOps, CI Analytics : Descriptive, Predictive (Added advantage) Tools : Jenkins and TFS Languages : Python, Java Enterprise Process Specific Skills : (Supervisory Responsibilities) : - When the Bigdata Architect is acting as an engagement or project manager, they will be responsible for the overall supervision of the staff on the engagement.- They will be responsible for coaching and advising the team on the engagement. They will likely need to provide feedback to the people managers and practice leads of the resources on their engagement team. Soft skills (Desired) :- Experience working with multi-divisional business communities to leverage information across the enterprise to improve business effectiveness - Strong team building, interpersonal, analytical, problem identification and resolution skills Soft Skills (Minimum) : - Exceptional analytical, conceptual, and problem-solving abilities - Strong written/oral communication and presentation/interpersonal skills - Highly self-motivated and able to work independently as well as in a team environment Education Requirements : Bachelor's degree or Master's degree Work Experience Requirements : - Minimum 10 years of professional experience with BI/DW implementations and with at least 2-3 years of Big Data Architecture. - 3+ years of experience in technical or solution architecture providing large scale enterprise data solutions. RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsDatabase MaintenanceTechnical ArchitectBig Data ArchitectBusiness IntelligenceHadoopBig DataDatabase DesignData AnalystData WarehousingData ModelingMapReduceSkills highlighted with ‘‘ are preferred keyskills","['Database Maintenance', 'Technical Architect', 'Big Data Architect', 'Business Intelligence', 'Hadoop', 'Big Data', 'Database Design', 'Data Analyst', 'Data Warehousing', 'Data Modeling', 'MapReduce']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleDatabase Architect / Designer,",Not Disclosed,12 - 22 years,Kolkata,"Job description12+ Years’ experience in Bigdata Space across Architecture, Design, Development, testing & Deployment, full understanding in SDLC.1. Experience of Hadoop and related technology stack experience2. Experience of the Hadoop Eco-system(HDP+CDP) / Big Data (especially HIVE) Hand on experience with programming languages such as Java/Scala/pythonHand-on experience/knowledge on Spark 3. Being responsible and focusing on uptime and reliable running of all or ingestion/ETL jobs4. Good SQL and used to work in a Unix/Linux environment is a must.5. Create and maintain optimal data pipeline architecture.6. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.7. Good to have cloud experience 8. Good to have experience for Hadoop integration with data visualization tools like PowerBI.RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsBig DataData ScienceAzureGCPData ArchitectHadoopCloudBigdataSparkETLAWSSkills highlighted with ‘‘ are preferred keyskills","['Big Data', 'Data Science', 'Azure', 'GCP', 'Data Architect', 'Hadoop', 'Cloud', 'Bigdata', 'Spark', 'ETL', 'AWS']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"IHXs vision is to build a Data Exchange and, develop AI/ML infused healthcare solutions delivering superlative experience to every stakeholder in the healthcare sector across all segments of the market. As we undertake this exciting journey together, we will be driven by a higher purpose to make healthcare availability, affordability and predictability a reality and believe that the solutions we develop and/or enable will leave a lasting impact on our society and country.","RoleData Engineer,","₹ 30,00,000 - 45,00,000 P.A. ",10 - 15 years,Bangalore/Bengaluru,"Job descriptionRoles and Responsibilities      Own, design, build, operationalize, secure, and monitor data processing systems     Translating business use cases across all products into technical specifications, including data     streams, integrations, transformations, databases, and data warehouses     Defining the data architecture framework, standards and principles, including modelling, metadata,     security     Evaluate tools/software required for development of data platform by conducting proof-of-concepts     Build logical and physical data models in sustainable, scalable manner and Design various ETL      processes defining data flows across data stores/systems     Collaborate with Product Owner, IT, business process owners and data scientists to     design/build/improve Data Warehouse and data marts     Be a Tech Anchor for the team on streaming & batch ETL development activities conducting code     reviews, and resolving technical issues and dependencies     Optimizing queries and building a scalable data model     Monitor processes and develop plans to capture and access all metadata, according to various ETL     processes.     Evaluate all proposals requests and improve structure of data warehouse     Ensure accuracy and quality of data movement across ETL pipelines in production environmentDesired Candidate Profile An SME with over 10+ years of deep expertise in building and understanding Data driven high availability applications that can be deployed to modern day vendor agnostic cloud infrastructures Proven experience in requirement gathering, Data curation and testing high performanceand distributed scalable computing solutions Hands on expertise in building and implementing data architecture for large enterprises. Real world experience of building scalable products that can withstand data volume of internet scale using technologies/frameworks such as Kafka, Spark, Kubernetes, Serverless and other parallel processing open-source and managed frameworks on large data volumes or data sets Well versed with Agile methodology for managing project deliverables. Someone who has traversed the data journey, having moved the clients through the modernization of data warehouses to data lakes to data operations on the cloud. Expertise in cloud-based data warehouse / data lake implementation (AWS/Azure) Hands-on experience in ETL, data warehouse, data lake, business intelligence, data governance, MDM, data virtualization and data ops technologies. Sound understanding of Relational, Graph and NoSQL data stores Proficient in testing Data pipeline and Analytics pipeline. Executed two or more engagements covering end to end lifecycle of nextgen data project (framing data strategy, define & drive new age data architecture, its transition to cloud and subsequent adoption) Consulted clients on aspiration to make them information led decision making business. Experience in deploying applications to a Cloud platform ( AWS/Azure) Ability to proactive pick up cutting edge trends in the industry Perks and Benefits Best In the IndustryRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Any SpecializationKey Skillsdata architectureAzure Data FactoryAzure DatabricksSkills highlighted with ‘‘ are preferred keyskills","['data architecture', 'Azure Data Factory', 'Azure Databricks']",['UG :B.Tech/B.E. in Any Specialization'],"Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 674,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at accenture.com","RoleTechnical Architect,",Not Disclosed,12 - 15 years,Bangalore/Bengaluru,"Job description  Project Role :  Data Architect    Project Role Description :  Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.    Management Level :  7    Work Experience :  12-15 years    Work location :  Bengaluru    Job Requirements :        Key Responsibilities :   1Function as the Lead Data Architect for a small, simple project/proposal or as a team lead for medium/large sized project or proposal2Discuss specific data architecture and data related issues with client architect/team in area of expertise3Analyze and assess the impact of the requirements on the data and its lifecycle4Lead the data architecture and design of complex, enterprise-level applications and systems5Breadth of experience in various client scenarios and situations     Technical Experience :   1Strong experience in Azure is preferred with hands-on experience in 2 or more of these skills : Azure SQL DB ,Azure SQL Managed Instance ,Azure Data Lake Store, Azure Cosmos DB, Azure Database for PostgreSQL, Azure Database for MySQL2Experience in handling medium to large data migration projects3 5 years of extensive database experience design build     Professional Attributes :   1Should be able to drive the technology design meetings, propose technology design and architecture 2Should have excellent client communication skills3Should have good analytical and problem-solving skills     Educational Qualification :   1Must have: BE/BTech/MCA 2Good to have: ME/MTech  RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :M.Tech, MCA in ComputersKey SkillsData migrationPostgresqlMySQLConsultingSQLArchitectureAnalyticalCosmosData architectureSkills highlighted with ‘‘ are preferred keyskills","['Data migration', 'Postgresql', 'MySQL', 'Consulting', 'SQL', 'Architecture', 'Analytical', 'Cosmos', 'Data architecture']","['UG :B.Tech/B.E.', 'PG :M.Tech, MCA in Computers']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"We Are Brillio. Born of the digital age in 2014. We are laser-focused on turning technological disruptions into the advantages that our customers need to thrive in todays digital economy.Brilliois a company focused on digital technologies andbig dataanalytics headquartered inSanta Clara, California, United States","RoleDatabase Architect / Designer,",Not Disclosed,6 - 10 years,Bangalore/Bengaluru,"Job description  BSc or MSc (preferred) in a STEM field Relevant work experience of 5 years Fluency in Python (especially Numpy and Pandas) and familiarity in PySpark Extensive hands-on experience with AWS Analytical Components like S3, EC2, Lambdas, Glue, SQS, SNS, DynamoDB, Redshift, RDS etc.    Experience with Data Lake Formation and Athena    Work Experience with industry standard distributed systems (ie    Spark, hive), data pipeline tools (ie    Airflow), NoSQL Databases (DynamoDB) and databases (PostgreSQL) Experience with Data Analysis, Significant experience optimizing data retrieval processes supporting API output, ideally within a low query volume / high data volume environment    Demonstrably deep experience with relevant big data processing either via Spark or through a modern MPP database like Redshift, ideally with experience in both Demonstrably deep experience with CI/CD tools and practices in a containerized AWS environment, from deployment pipelines (Jenkins, etc), infrastructure definition (Terraform, CloudFormation, etc    Understand and design for non-functional concerns such as performance, cost optimization, maintainability, and developer experience.             RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData analysisNoSQLAnalyticalPostgresqlData ArchitectData processingbig dataDistribution systemCost optimizationPython","['Data analysis', 'NoSQL', 'Analytical', 'Postgresql', 'Data Architect', 'Data processing', 'big data', 'Distribution system', 'Cost optimization', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"A Silicon-Valley headquartered company, Infogain is a global business oriented IT consulting provider of front-end, customer-facing technologies, processes and applications, leading to a more efficient and streamlined customer experience. We want our clients€™ interactions with their customers to be fast, efficient, and cost effective.With close to 4,000 employees in the United States, India, the Middle East, U.K., Singapore and Malaysia, we service 5 of the world€™s largest 50 companies, and 24 of the Fortune 500. we have million-dollar engagements with over 25 customers, many of which have been with us for 5 years or more.At Infogain, we place a high value on establishing long-term relationships with our clients, ultimately becoming virtual extensions of their organizations. In fact, more than 90% of our medium and large deal size clients from five years ago remain clients today. Why? Our consultants, project managers and engineering teams listen and address our clients€™ specific requirements with best-in-class solutions across a broad spectrum of service areas.Infogain is an Oracle Knowledge Management Expert, having the world€™s largest practice to improve customer support operations and boost satisfaction in High Tech, Insurance, Travel & Hospitality and Retail.We are also a global leader in Oracle Retail€™s customer facing products.For insurance companies, Infogain helps make the claims process more efficient, effective and customer friendly.","RoleData Engineer,",Not Disclosed,8 - 11 years,Noida,"Job descriptionGreeting form Infogain! We are having Immediate requirement for Azure Data Architect in Infogain India Pvt Ltd. Please Find the Job Description below & If you are interested please share your updated Resume with details:-Mode of Hiring-PermanentExperince-8-12 YrsSkills Required- ADF, SQL, MS SQL Server, SynapseNotice Period- Immediate to 30 Days MaxLocation- Noida/Bangalore/Pune/Mumbai (Currently work from home)Job Description 6+ Years experience in Azure Stack(ADF, ADLS, Synapse, Databricks, PowerBI) and strong Data Warehouse, SQL DB, Data Modelling Skills. Hands-on exposure to coding and mentor team on technical abilities Kindly share your update word formatted /pdf profile with the details below on arti.sharma@infogain.com Total exp-Exp in ADFExp in SynapseExp in DatabricksCurrent CTC:Exp CTC:NP -Current Location-Preferred Location- About Infogain:Infogain is a Silicon Valley headquartered company with expertise in software platform engineering and deep domain skills in travel, retail, insurance, automotive, and high technology. We accelerate the delivery of digital customer engagement systems using digital technologies such as cloud, microservices, robotic process automation, and artificial intelligence for our clients. Our unique engagement approach of Listen-Curate-Deliver helps to accelerate the innovation journey of 5 of the worlds largest 50 companies and 24 of the Fortune 500, with several relationships of over 10 years. We deliver positive business outcomes using rapid prototyping and a solid foundation of DevOps-based software platform engineering that ensure high-quality and on-time delivery. Our 3,500 global employees across the US, UK, Singapore, Middle East and India focus on client value creation, delivery excellence and innovation. Our locations in India have 2700 employees spanning Noida, Pune, Mumbai and Bangalore.Infogain maintains both strategic and technology partnerships with leading enterprise software providers to deliver value-added solutions. We engage with the world's largest, as well as mid-size, and startup, software providers for building product capability, product marketing, customization, professional services and post-implementation supportRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Any DoctorateKey SkillsAzure Data FactoryADFSkills highlighted with ‘‘ are preferred keyskills","['Azure Data Factory', 'ADF']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Any Doctorate']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 674,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at accenture.com","RoleTechnical Architect,",Not Disclosed,6 - 8 years,Bangalore/Bengaluru,"Job description  Project Role :  Data Architect    Project Role Description :  Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.    Management Level :  9    Work Experience :  6-8 years    Work location :  Bengaluru    Job Requirements :        Key Responsibilities :   aFunction as the Lead Data Architect for a small, simple project/proposal or as a team lead for medium/large sized project or proposalbDiscuss specific Big data architecture and related issues with client architect/team in area of expertisecAnalyze and assess the impact of the requirements on the data and its lifecycledLead Big data architecture and design medium-big Cloud based, Big Data and Analytical Solutions using Lambda architectureeBreadth of experience in various client scenario     Technical Experience :   aStrong experience in Azure is preferred with hands-on experience in two or more of these skills : Azure Synapse Analytics, Azure HDInsight, Azure Databricks with PySpark / Scala / SparkSQL, Azure Analysis ServicesbExperience in one or more Real-time/Streaming technologies including: Azure Stream Analytics, Azure Data Explorer, Azure Time Series Insights, etccExperience in handling medium to large Big Data implementationsdFor Level 8 - Candidate must have 10-12 years of IT experience     Professional Attributes :   aShould be able to drive the technology design meetings, propose technology design and architecture bShould have excellent client communication skillscShould have good analytical and problem-solving skills     Educational Qualification :   aMust have: BE/Btech/MCA bGood to have: ME/Mtech  RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :MCA in Computers, M.TechKey SkillsAnalyticalConsultingSCALAAnalyticsData architectureArchitecturebig dataSkills highlighted with ‘‘ are preferred keyskills","['Analytical', 'Consulting', 'SCALA', 'Analytics', 'Data architecture', 'Architecture', 'big data']","['UG :B.Tech/B.E.', 'PG :MCA in Computers, M.Tech']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Triangle ( A Unit of Ratein Infotech India Private Ltd) is a Recruitment Specialist Organisation with focus on Information Technology Industry. Incorporated in August 2000 with 4 offices in India at Mumbai, Pune, New Delhi & Bangalore, Triangle caters to more than 25 active clients who are Fortune 500 organizations and world¢??s largest IT organizations.  Our Core expertise is in End to End Permanent and Contract Staffing and Extended Services.  We have been Internationally recognized as WeConnect Certified woman owned and managed organization.  Triangle has featured in CIO Review Magazine as ¢??Company of the Year in Staffing and Recruitment¢? Category for year 2014.","RoleSolution Architect,",Not Disclosed,10 - 15 years,Bangalore/Bengaluru,"Job description   Experience in architecting with AWS or Azure Cloud Data Platform Successfully implemented large-scale data warehouse / data lake solutions in snowflake or AWS Redshift Be proficient in Data modelling and data architecture design experienced in reviewing 3rd Normal Form and Dimensional models    Experience in implementing Master data management, process design and implementation    Experience in implementing Data quality solutions including processes    Experience in IOT Design using AWS or Azure Cloud platforms    Experience designing and implementing machine learning solutions as part of high-volume data ingestion and transformation    Experience working with structured and unstructured data including geo-spatial data    Experience in technologies like python, SQL, no SQL, KAFKA, Elastic Search Hands on experience using snowflake, informatica, azure logic apps, azure functions, azure storage, azure data lake and azure search    Behaviors Required: Driven by our values and purpose in everything we do Visible, active, hands on approach to help teams be successful Strong proactive planning ability    Optimistic, energetic, problem solver, ability to see long term business outcomes    Collaborative, ability to listen, compromise to make progress    Stronger together mindset, with a focus on innovation & creation of tangible / realized value      Education      Qualifications, Accreditation, Training: Required: Degree in Computer Science and/or related fields Azure or AWS Solution Architecture certifications    RoleSolution Architect,Industry TypeRecruitment / Staffing,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsComputer scienceSolution architectureProcess designMachine learningData ArchitectData qualityInformaticaSQLPythonData architecture","['Computer science', 'Solution architecture', 'Process design', 'Machine learning', 'Data Architect', 'Data quality', 'Informatica', 'SQL', 'Python', 'Data architecture']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeRecruitment / Staffing,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Eli Lilly and Company strives to create informative and engaging online communities that share information in meaningful ways. We welcome and encourage your participation and engagement in thoughtful and respectful discourse.,"RoleTechnical Architect,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job description       Deep technical understanding of various data architectures and solution options along with an understanding of business requirements.    Deep understanding of cloud technologies and strategies including knowledge on cloud infrastructure (Azure/ AWS)    Ability to create roadmaps through working with multiple stakeholders Global Technology Teams and Business    Translating requirements into technical design and architecture, aligned with enterprise and industry standards and best practices, along with the corresponding required documentation.    Well-versed in both the evolving technology landscape as well as the needs of the business areas they support.    Has a deep understanding of data sources, flows, integration, acquisition, structures, quality, harmonization, and governance.    Translate business requirements into consumable data models and source to target data mapping.    Consult with internal customers who own the data to develop information relationships that lead to actionable insights.    Work with data from multiple data sources to build integrated views that will drive decisions.    Work in Agile Sprints with business facing project teams.      Additional Responsibilities        Collaborates with architects on technical design issues, particularly for solutions that impact local and global business areas    Work with Global Engineering Business Partners and Sites to understand business problems and set IT direction roadmap for solutions to meet business needs    Work with central and site engineering business partners and define processes for capture and maintaining engineering data, such as equipment metadata, supporting to the goal of creating an equipment digital twin for key process equipment sets and integration of this metadata with various sources of data.    Perform data issue analysis and work with various MQ teams to improve data quality confidence.    Serve as the technical liaison between the Bigdata Engineering admins, Data Scientists, Data Engineers and Product Owners.    Validate data quality coverage and accuracy by developing reports and tools to monitor and visualize data quality    Support the effort in finding data quality gaps by working with business and across IT domains to identify process or data management changes.    Work with Data Integration Developers to build solutions.    Work closely with the Data Integration and Analytics team to ensure we are in sync with the NextGen data lake    Develop deep understanding of technologies that enable business processes within key areas.    Develop solid understanding of business processes and needs to support recommended technical direction.    Responsible for ensuring adherence to technology roadmaps driven by the Lilly Enterprise Architecture community.    Drives decisions with respect to technologies and use of those technologies.    Ensure interface and data needs are understood and that the necessary technology and architecture are in place to meet these needs.    Responsible for understanding, influencing, and evolving the supported application(s) technical design.    Understanding of current technology as well as future direction for the area and the enterprise.        Basic Requirements      BachelorDegree and 5 yearsexperience in the implementation of modern data ecosystems.    Expertise in Python, SQL, Hadoop, Hive, Spark, UNIX Shell scripting, and Informatica big data technologies.    Expertise with JSON, REST API, and other data integration technologies.    Experience in one or more cloud-based data solutions/ cloud infrastructure    Demonstrated understanding of data privacy and CCI requirements and experience delivering within those requirements.    Experience in applying quality and compliance requirements.    Excellent oral and written communication skills.    Fluency in English.    A high level of intellectual curiosity, external perspective, and innovation interest    Strong analytical, problem solving and investigative skills    Experience with security models and development on large data sets    Excellent self-management skills.    RoleTechnical Architect,Industry TypePharmaceutical & Life Sciences,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsComputer scienceData managementAgileHealthcareJSONInformaticaUnix shell scriptingAnalyticsSQLPython","['Computer science', 'Data management', 'Agile', 'Healthcare', 'JSON', 'Informatica', 'Unix shell scripting', 'Analytics', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePharmaceutical & Life Sciences,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Eli Lilly and Company strives to create informative and engaging online communities that share information in meaningful ways. We welcome and encourage your participation and engagement in thoughtful and respectful discourse.,"RoleDatabase Architect / Designer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job description       Minimum 5 years Pharmaceutical Manufacturing experience.Knowledge of GxP, Supply Chain, Pharmaceutical manufacturing processes and automations systems    Minimum 5 years of relevant experience with data architecture design    Experience in Computer System Validation processes and Data Integrity concepts    Excellent communication, interpersonal, and influencing skills.    Ability to carry on conversations with business customers, upper management, and technical personnel.    Experience in working in an international and multicultural environment where team members are spread into different countries    Ability to thrive in a complex, changing, virtual environment with multiple competing priorities    Strong problem solving and organizational skills    A high level of intellectual curiosity, external perspective, and innovation interest    Strong analytical, problem solving and investigative skills    Experience with security models and development on large data sets    Demonstrated understanding of data privacy and CCI requirements and experience delivering within those requirements.    Excellent self-management skills.          Technology Background:      Experience with Agile Methodology    ITIL certification    Knowledge of best practices for code review and management in GitHub    Experience in design and development of data lakes, data warehouses, data marts and schema design    Building and maintaining complex data pipelines    Expertise in performing data extraction, profiling, cleansing, conversion, transformation and loading of data    Knowledge of CI/CD, Infrastructure as Code and more generally Everything as Code    Experience with DBMS software and tools (PostgreSQL, Oracle, Snowflake, Teradata, MS SQL Server, MongoDB, Toad, Hadoop, CouchDB, etc.)    Expertise in data management (structured/unstructured), data mining and reporting technologies    Extensive experience with data modeling using Erwin, PowerDesigner, Toad or similar software    Experience working with Amazon AWS, Microsoft Azure, Spark framework    Experience with a multitude of AWS services; especially Glue, DMS, CloudFormation, IAM, Route53, ALB/ELB, VPC, EC2, Lambda, S3, KMS, CloudTrail, Config, CloudWatch    Experience with Azure services; especially Azure Data Factory, Azure Resource Manager, Azure Synapse, Azure Functions, Azure Data Lake Analytics, Azure Data Lake Storage, Azure Monitor    Experience with SQL, PL/SQL, Python, Spark Framework, YAML, JSON, Chef    Experience with Docker, NodeJS, Ansible, Docker, Jenkins, Unix, Linux    Strong understanding of cybersecurity operation principles    Knowledge of industry standard NIST framework        Additional Preferences:      Knowledge of data science technologies like R, Keras or TensorFlow    Knowledge of ML/AI and natural language processing    Experience implementing and leading data governance    Experience working in a regulated environment    Manufacturing Operations experience preferred    Pharma Manufacturing experience strongly desired.    Multi-Site, Global IT Project Experiences strongly desired    RoleDatabase Architect / Designer,Industry TypePharmaceutical & Life Sciences,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateKey SkillsUnixMS SQLLinuxData modelingData ArchitectPLSQLJSONOracleTeradataPython","['Unix', 'MS SQL', 'Linux', 'Data modeling', 'Data Architect', 'PLSQL', 'JSON', 'Oracle', 'Teradata', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePharmaceutical & Life Sciences,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"ICON CLINICAL RESEARCH INDIA PRIVATE LIMITED is located in Chennai, Tamil Nadu, India and is part of the Scientific Research and Development Services Industry. ICON CLINICAL RESEARCH INDIA PRIVATE LIMITED has 2,444 total employees across all of its locations. There are 10 companies in the ICON CLINICAL RESEARCH INDIA PRIVATE LIMITED corporate family.","RoleTechnical Architect,",Not Disclosed,5 - 9 years,Bangalore/Bengaluru,"Job description  As a    Data Architect II   you will be responsible for helping to maintain and optimize the data analysis infrastructure that is used to drive the processes necessary to allow the Medical Informatics team to provide Real World Evidence solutions that support various critical aspects of clinical research to ICON s internal and external stakeholders.      Responsibilities     Efficiently move large volumes of data from diverse data platforms into Hadoop platform   Work with large volumes of data so as to derive Business Intelligence   Write scalable and maintainable ETLs   Managing Hadoop jobs using scheduler   Write solid, adaptable, and high-performing code   Strong understanding of Impala, Hive, HBase, Pig, and Sqoop   Good understanding of Linux script          What you need:      Degree qualified - Computer Science or other STEM discipline     Healthcare RWD experience is essential for this role. 3+ years experience working with transactional health data such as medical, prescription, and EMR/EHR is required.     Relational database and/or DFS experience, specifically: 5+ years of experience in Hadoop (Impala/Hive)   Programming skills, specifically: 3+ years of experience in SQL/HPLSQL ; 2+ years of experience in Python, Java, C#   Platform flexibility specifically: 3+ years of experience in Linux (SSH/SFTP)   RoleTechnical Architect,Industry TypePharmaceutical & Life Sciences,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsDFSData analysisLinuxPharmaData ArchitectClinical researchHealthcarePublic healthSQLPython","['DFS', 'Data analysis', 'Linux', 'Pharma', 'Data Architect', 'Clinical research', 'Healthcare', 'Public health', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePharmaceutical & Life Sciences,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Triangle ( A Unit of Ratein Infotech India Private Ltd) is a Recruitment Specialist Organisation with focus on Information Technology Industry. Incorporated in August 2000 with 4 offices in India at Mumbai, Pune, New Delhi & Bangalore, Triangle caters to more than 25 active clients who are Fortune 500 organizations and world¢??s largest IT organizations.  Our Core expertise is in End to End Permanent and Contract Staffing and Extended Services.  We have been Internationally recognized as WeConnect Certified woman owned and managed organization.  Triangle has featured in CIO Review Magazine as ¢??Company of the Year in Staffing and Recruitment¢? Category for year 2014.","RoleTechnical Architect,",Not Disclosed,10 - 15 years,Bangalore/Bengaluru,"Job description    Azure Data Architect with streaming experience Total experience in data management area for 10 + years with Azure cloud data platform experience        Architect with Azure stack (ADLS, AALS, Azure Data Bricks, Azure Streaming        Analytics Azure Data Factory, cosmos DB Azure synapse) mandatory expertise on Azure streaming Analytics ,Data Bricks, Azure synapse, Azure cosmos DB        Must have worked experience in large Azure Data platform and dealt with high volume Azure streaming Analytics        Experience in designing cloud data platform architecture, designing large scale environments        5 plus Years of experience architecting and building Cloud Data Lake, specifically Azure Data Analytics technologies and architecture is desired,      Enterprise Analytics Solutions. ‚Leading development of Data Lake Architectures from scratch for streaming Analytics Platform ‚5+ years of Programming experience in Python, SQL, Spark ‚Experience on Azure data explorer (ADX) is good to have it       RoleTechnical Architect,Industry TypeRecruitment / Staffing,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData managementArchitectural designData ArchitectCloudProgrammingData analyticsArchitectingCosmosSQLPython","['Data management', 'Architectural design', 'Data Architect', 'Cloud', 'Programming', 'Data analytics', 'Architecting', 'Cosmos', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeRecruitment / Staffing,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
" Thanks & Best Regards,Hemalatha| Lead Talent Acquisition  www.cygnuspro.comCygnusPro Software Solutions Pvt. Ltd.Tel:     04068196805Mob:  9989759380email:  hemalatha@cygnuspro.com","RoleData warehouse Architect / Consultant,",Not Disclosed,15 - 20 years,Bangalore/Bengaluru,"Job description  Exp  15 + Yrs ,  Expertise in IICS Data Integration & App Integration                              Good Data Solutioning  Experience                              Exposure to Snowflake / Data Bricks                              Exposure to Java / Python will be added advantage                                Strong stakeholder management & Communication skills                              Exposure to Informatica EDC / Axon & MDMRoles and Responsibilities Desired Candidate Profile   Exp  15 + Yrs ,  Expertise in IICS Data Integration & App Integration                              Good Data Solutioning  Experience                              Exposure to Snowflake / Data Bricks                              Exposure to Java / Python will be added advantage                                Strong stakeholder management & Communication skills                              Exposure to Informatica EDC / Axon & MDMPerks and Benefits RoleData warehouse Architect / Consultant,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, Temporary/ContractualRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData SolutioningIICS Data Integration and App IntegrationInformatica EDC / Axon & MDMJava / PythonSnowflake / Data BricksSkills highlighted with ‘‘ are preferred keyskills","['Data Solutioning', 'IICS Data Integration and App Integration', 'Informatica EDC / Axon & MDM', 'Java / Python', 'Snowflake / Data Bricks']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Temporary/Contractual",Role CategoryDBA / Data warehousing
"Established in the year 2017, with a view to providing Development and Staffing Solutions. We are driven by the goal to provide services that change lives. We are evolving as the manifestation of our founders, where we resolve complex development and staffing issues of our clients with strong ethical and proficient foundation.","RoleDatabase Architect / Designer,",Not Disclosed,7 - 10 years,Bangalore/Bengaluru,"Job description You should have expertise in designing, implementing, and operating stable, scalable, solutions to flow data from production systems into analytical data platform (big data tech stack + MPP) and into end-user facing applications for both real-time and batch use cases.  The ability to do deep problem solving and build elegant, maintainable solutions to complex problems. Designing platforms as consumable data services across the organization using Big Data tech stack. Do high level design independently; Functional modelling, break-down of a module. Influence product requirements & operational plans. Instil best practices for development and champion their adoption, while working with product managers to estimate and plan projects in agile development framework.  Architectural & Design Choices, Deep knowledge on one or more tech stacks, identify alternative tech choices and trade-offs. Build and execute data modeling projects across multiple tech stacks i.e. big data, MPP, OLAP using agile development techniques. Strong problem Solving skills, Identify feasible alternatives and freeze on the optimal choice of design approach. Strong engineering mind set - build automated monitoring, alerting, self-healing (restart ability/graceful failures) features while building the consumption pipelines.   Challenge status quo and propose innovative ways to process, model, consume data when it comes to tech stack choices or design principles. Translate business requirements into technical specification (fact/dimension/filters/derivations/aggregations).   Lead by example, mentor and guide team members on everything from structured problem solving to development of best practices.  An ideal candidate will have excellent communication skills to be able to work with engineering, product and business owners to develop and define key business questions and to build data sets that answer those questions. you should bring your passion for working with huge data sets and bringing datasets together to answer business questions and drive change.RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :B.Tech/B.E. in Computers, BCA in Computers, B.Sc in ComputersPG :Any PostgraduateDoctorate :Doctorate Not RequiredKey Skillsbig dataHiveHadoopOLAPSparkMPPMap reduceScala programmingcommunication skillsSkills highlighted with ‘‘ are preferred keyskills","['big data', 'Hive', 'Hadoop', 'OLAP', 'Spark', 'MPP', 'Map reduce', 'Scala programming', 'communication skills']","['UG :B.Tech/B.E. in Computers, BCA in Computers, B.Sc in Computers', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"Our Company is a guaranteed for its exceptional ELDF ( Enterprise Level Solution Development Firm ) for giving the particular services to different businesses with numerous physio-graphic puts over the world. We make the association for the intellectual business by working with up and coming latest advancements to serve the best client experience through customized benefits and upgraded collaboration.We are having a range of products like Customized ERP, Smart Dashboard, Audit, CRM and IOT.","RoleTechnical Architect,",Not Disclosed,8 - 15 years,Kolkata,"Job description     Good Understanding of Distributed Data Platforms.    Should have worked as data architect in Implementing a medium/large scale Data Warehouse solution.    Experience in Migrating Legacy Data Warehousing Solution to GCP Cloud.    Deep exposure hands-on GCP Cloud Native ETL / ELT services with deep understanding of BigQuery and Looker or any other reporting platform.    Possess in depth knowledge and hands on development experience operationalizing large scale ingestion, processing, consumption using either DataProc or Dataflow or cloud fusion.    Strong understanding and experience with Storage infrastructure, event-based architecture using Cloud Functions, Monitoring, Logging, Auditing services of GCP.    Strong experience on either one or more MPP Data Warehouse Platforms prefer BigQuery, CloudSQL, Cloud Spanner, Fire store or similar.    Strong Development Experience on at least one or more event-driven streaming platforms prefer PUB/SUB, Kafka   Exposure to Networking on GCP and Gateway connectivity.    Strong Data Orchestration experience using tools such has Cloud Functions, Dataflow, Cloud Composer, Apache Airflow or related.   RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsSUBorchestrationNetworkingGCPCloudData Architectcloud storageApacheData warehousingMonitoring","['SUB', 'orchestration', 'Networking', 'GCP', 'Cloud', 'Data Architect', 'cloud storage', 'Apache', 'Data warehousing', 'Monitoring']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Triangle ( A Unit of Ratein Infotech India Private Ltd) is a Recruitment Specialist Organisation with focus on Information Technology Industry. Incorporated in August 2000 with 4 offices in India at Mumbai, Pune, New Delhi & Bangalore, Triangle caters to more than 25 active clients who are Fortune 500 organizations and world¢??s largest IT organizations.  Our Core expertise is in End to End Permanent and Contract Staffing and Extended Services.  We have been Internationally recognized as WeConnect Certified woman owned and managed organization.  Triangle has featured in CIO Review Magazine as ¢??Company of the Year in Staffing and Recruitment¢? Category for year 2014.","RoleTechnical Architect,",Not Disclosed,10 - 15 years,Bangalore/Bengaluru,"Job description    Azure Data Architect with streaming experience Job description Total experience in data management area for 10 + years with Azure cloud data platform experience Architect with Azure stack (ADLS, AALS, Azure Data Bricks, Azure Streaming Analytics Azure Data Factory, cosmos DB Azure synapse) mandatory expertise on Azure streaming Analytics ,Data Bricks, Azure synapse, Azure cosmos DB Must have worked experience in large Azure Data platform and dealt with high volume Azure streaming Analytics Experience in designing cloud data platform architecture, designing large scale environments 5 plus Years of experience architecting and building Cloud Data Lake, specifically Azure Data Analytics technologies and architecture is desired, Enterprise Analytics Solutions, and optimizing real time big data data pipelines, architectures and data sets     Job Description Architecting Microsoft Azure Solutions across multiple platforms    Implementation and Delivery of Microsoft Azure projects    Documentation of solutions (eg architecture, configuration and setup)    Working within a project management/agile delivery methodology in a leading role as part of a wider team    Experience of setting up, deploying and managing multiple environments to support agile development approaches hands-on experience with DevOps toolset    Knowledge of PowerShell, Git, ARM templates and deployment automation 10+ years of industry experience   Designing / Developing / Envisioning Enterprise apps using Azure IaaS and Analytics services with a focus on Azure RBAC, Azure AD, Azure Data Factory, Azure Data Lake Storage, Azure Function App, Azure Data Warehouse, Azure SQL DB, Azure Databricks      RoleTechnical Architect,Industry TypeRecruitment / Staffing,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsAutomationGITData managementPowershellProject managementAgile developmentData ArchitectArchitectural designCosmosSQL","['Automation', 'GIT', 'Data management', 'Powershell', 'Project management', 'Agile development', 'Data Architect', 'Architectural design', 'Cosmos', 'SQL']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeRecruitment / Staffing,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleTechnical Architect,",Not Disclosed,11 - 14 years,Bangalore/Bengaluru,"Job descriptionJob Description : Big Data Architect Function : Data Integration Location : Off Shore / India Basic Function : - The Big Data Architect works closely with the customer and the solutions architect to translate the customer's business requirements into a Big Data solution. - This includes understanding the customer data requirements, platform selection, design of the technical architecture, design of the application and interfaces, and development, testing, and deployment of the proposed solution. - Has the ability to design enterprise grade large-scale data processing systems and help identify the best options for architecture. - The Big Data Architect also understands the complexity of data and can design systems and models to handle different variety of data with varying levels of volume, velocity and veracity. - Should have independently worked on proposing architecture, design and data ingestion concepts in a consultative mode. - Leads client assessments, preparing current state and future state architectures along with go forward recommendations. - Will work with the practice leads and account management team to develop statements of work, implementation plans, resource plans and project estimates Essential Functions : - Has a deep understanding and experience with several of the following: Business Analysis, Requirements Gathering, Data Analysis, Data Modeling, Project Management, Project Estimation - Advanced knowledge of design and architecture patterns and methodologies. - Demonstrated work ethic, focus and self-discipline - Has and maintains a deep understanding of the role of big data in business and the enterprise. - Propose recommended and/or best practices regarding the movement, manipulation, and storage of data in a big data solution including data ingestion, data storage options, query techniques, data variety, volume & velocity, - Collaborate with project teams on platform development process from inception to production deployment, including project scheduling, design, implementation and coordination with other team members. - Collaborate with other technology teams and architects to define and develop cross- function technology stack interactions. - Research and experiment with emerging technologies and tools related to big data. - Experience in scaling applications on big data platforms to massive size. - Performing solution architecture in adherence to enterprise architecture governance. - Bridging business and development team. - Long term development and Technical expertise in DW/BI Practice, communicate well with all stakeholders, optimize objectives, leverage state of the art tools and best practices, integrate into corporate systems and deliver on time. - Deep understanding in Data Warehousing, Enterprise Architectures, Dimensional Modelling, Star & Snow-flake schema design, Reference DW Architectures, ETL Architect, ETL (Extract/Transform/Load), Data Analysis, Data Conversion/Transformation, Database Design, Data Warehouse Optimization, Data Mart Development, and Enterprise Data Warehouse Maintenance and Support etc. - Should have independently worked on proposing architecture, design and data ingestion concepts. Primary Internal Interactions : Project Delivery : - Formulates workable solutions that integrate people, process and technology. Integrates competency leads and other subject matter experts to define the details of vision and solution - Identifies, creates and communicates the vision of the end-to-end solution for the client and implementation teams after completing thorough evaluations of the present state needs - Leads technical teams in documenting and implementing technical solutions for business problems - Supports Delivery Enablement and Practice Leads by ensuring appropriate linkage between practices is clear and well understood - Works with practice leads to develop project estimating methodologies for new areas of focus Account & Resource Management : - Partners with the account teams to identify and sell solutions and create high level of client interest in proposed comprehensive solutions - Works closely with Engagement Managers, Account Managers and Account Leads in strategies to grow existing accounts - Partners with account team and client to develop the ROI, Business Case, and Statement of Work for an engagement - Develops comprehensive project, implementation, and resource plans working closely with Engagement and Project Managers Primary External Interactions : Assessment Participation & Leadership : - Identifies, creates and communicates the vision of the end-to-end solution for the client and implementation teams after completing thorough evaluations of the present state needs - Participates in strategy development and current state assessments for clients to identify opportunities. Acts as proxy for practice leads when necessary - Works with client to sell the proposed solution and develop internal marketing programs - Supports work that may not fall into an existing competency and helps to identify possible needs for new competencies. - Partners with account team and client to develop the ROI, Business Case, and Statement of Work for an engagement Organizational Relationships : Reports To : EDM India Practice Lead - over time, this will evolve Supervises : NA Skills : Technical Skills (Knowledge, Skills & Abilities) : - Experience with enterprise data management, Business Intelligence, data integration, and SQL database implementations - Experience with the major big data solutions like Hadoop, MapReduce, Hive, Spark, Scala, HBase, MongoDB, Cassandra. - Programming/scripting languages like Java, Linux, PHP, Ruby, Python and/or R. As well as have experience in working with ETL tools such as Informatica, Talend, Pentaho etc. - He or she should have experience in designing solutions for multiple large data warehouses with a good understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS and/or knowledge on NoSQL platforms. - Experience in data migration from relational databases to Hadoop HDFS - Propose best practices/standards - Translate, load and present disparate datasets in multiple formats/sources including JSON, XML etc. Must Have Skills : - Hadoop stack including HDFS cluster, MapReduce, Hive, Spark and Impala - Web Technologies CSS, DHTML, XML, Hight Charts, Linux - ETL tools such as Informatica, Talend and/or Pentaho. Query : SQL, No SQL Concepts Ingest : Kafka, Sqoop, Flume Orchestration : Zookeeper Databases : Postgres, Mongo DB, Cassandra, HBase Languages : Java, Scala Scripting : JavaScript, DHTML, XML, Shell Good to have Skills : Core : AWS, Hadoop, Yarn Process : Agile-Scrum, Iterative Development, DevOps, CI Analytics : Descriptive, Predictive (Added advantage) Tools : Jenkins and TFS Languages : Python, Java Enterprise Process Specific Skills : (Supervisory Responsibilities) : - When the Bigdata Architect is acting as an engagement or project manager, they will be responsible for the overall supervision of the staff on the engagement.- They will be responsible for coaching and advising the team on the engagement. They will likely need to provide feedback to the people managers and practice leads of the resources on their engagement team. Soft skills (Desired) :- Experience working with multi-divisional business communities to leverage information across the enterprise to improve business effectiveness - Strong team building, interpersonal, analytical, problem identification and resolution skills Soft Skills (Minimum) : - Exceptional analytical, conceptual, and problem-solving abilities - Strong written/oral communication and presentation/interpersonal skills - Highly self-motivated and able to work independently as well as in a team environment Education Requirements : Bachelor's degree or Master's degree Work Experience Requirements : - Minimum 10 years of professional experience with BI/DW implementations and with at least 2-3 years of Big Data Architecture. - 3+ years of experience in technical or solution architecture providing large scale enterprise data solutions. RoleTechnical Architect,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsDatabase MaintenanceTechnical ArchitectBig Data ArchitectBusiness IntelligenceHadoopBig DataDatabase DesignData AnalystData WarehousingData ModelingMapReduceSkills highlighted with ‘‘ are preferred keyskills","['Database Maintenance', 'Technical Architect', 'Big Data Architect', 'Business Intelligence', 'Hadoop', 'Big Data', 'Database Design', 'Data Analyst', 'Data Warehousing', 'Data Modeling', 'MapReduce']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleDatabase Architect / Designer,",Not Disclosed,12 - 22 years,Kolkata,"Job description12+ Years’ experience in Bigdata Space across Architecture, Design, Development, testing & Deployment, full understanding in SDLC.1. Experience of Hadoop and related technology stack experience2. Experience of the Hadoop Eco-system(HDP+CDP) / Big Data (especially HIVE) Hand on experience with programming languages such as Java/Scala/pythonHand-on experience/knowledge on Spark 3. Being responsible and focusing on uptime and reliable running of all or ingestion/ETL jobs4. Good SQL and used to work in a Unix/Linux environment is a must.5. Create and maintain optimal data pipeline architecture.6. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.7. Good to have cloud experience 8. Good to have experience for Hadoop integration with data visualization tools like PowerBI.RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsBig DataData ScienceAzureGCPData ArchitectHadoopCloudBigdataSparkETLAWSSkills highlighted with ‘‘ are preferred keyskills","['Big Data', 'Data Science', 'Azure', 'GCP', 'Data Architect', 'Hadoop', 'Cloud', 'Bigdata', 'Spark', 'ETL', 'AWS']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
