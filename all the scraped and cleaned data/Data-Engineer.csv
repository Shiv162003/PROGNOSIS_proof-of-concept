company,role,salary,experience,Location,description,skills,qualification,industry_type,Functional_area,Employment_type,Role_category
"Vedantu is the pioneer of LIVE Online Learning in India and has reddened online learning since. Our aim is to make quality learning accessible to students in all parts of the country, from the comfort of their homes. We have students from 1000+ Cities and 30+ Countries studying from teachers across 300+ cities ","RoleData Engineer,",Not Disclosed,4 - 9 years,Bangalore/Bengaluru,"Job description     Roles and Responsibilities  Data Platform Engineering at Vedantu builds distributed components, systems, and tools that power decisions at ‘Vedantu’. We have an incredibly rich dataset to collect, transform, and analyze in order to improve the effectiveness of our marketplace and create delight for our customers. The data scientists, machine learning engineers, tech engineers and analysts use this data to make the experience of using 'Vedantu' Platform better for the customers. We leverage existing open source technologies like Kafka, Hadoop, Hive, Presto/Trino, Data Lake and also write our own. As a member of our team you would spend time designing and growing our existing Data Warehouse, democratising data access at the company, and promoting the correct use of data and analytics at the company.   As an integral part of the Data Platform team, take ownership of multiple modules from      design to deployment.  Extensively build scalable, high-performance distributed systems that deal with large data  volumes.  Evaluating and understanding various data sources and data access techniques. Data sources include S3, No SQL databases, shared disk storage, Relational databases, Data Lake, Streaming Data etc. Understand the concepts and principles of query federation, data modeling and can produce, maintain and update relevant data models across multiple subject areas. Collaborate with different teams in order to understand / resolve data availability and consistency issues. Experience in designing metadata repositories, understanding a range of metadata tools and technologies to implement metadata repositories and working with metadata. ] Provide solution improvement and deployment best practices on the Data-Modeling platform. Responsible for deploying solutions following data industry standard compliances, regulation and guidelines. Desired Candidate Profile   Understanding of Big Data Engineering/processing, Business Intelligence and Advanced   analytics.  Knowledge in databases and Data Warehouse modeling, Solid understanding of SQL and  databases (Mysql, Postgres etc.), including SQL and NoSQL.  Experience in Trino (Presto) distributed SQL engine.  Technical expertise with data models, data pipeline, streaming analytics and advanced   analytics.  Expert in programming languages  KSQL, Spark SQL, Streaming/ Hive, Presto, Apache Iceberg and Data Lake.  Experience in accessing data from various data sources  AWS S3, Relational Databases, Data  Lakes, No SQL databases etc.  Knowledge of docker, Kubernetes, AWS EKS and other AWS services.  Be agile and ready take new challenges  Work in multi-functional groups with diverse interests and requirements to achieve a common goal. Perks and Benefits RoleData Engineer,Industry TypeE-Learning / EdTech,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E. in Any SpecializationKey SkillsData WarehousingStreamingflinkhiveData EngineeringKafkaData ModelingSparksenior data engineerSQLprestoSkills highlighted with ‘‘ are preferred keyskills","['Data Warehousing', 'Streaming', 'flink', 'hive', 'Data Engineering', 'Kafka', 'Data Modeling', 'Spark', 'senior data engineer', 'SQL', 'presto']",['UG :B.Tech/B.E. in Any Specialization'],"Industry TypeE-Learning / EdTech,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleData Engineer,",Not Disclosed,7 - 9 years,Hyderabad/Secunderabad,"Job descriptionUsing services and tools to ingest, egress, and transform data from multiple sources IN depth technical knowledge of tools like Azure Data factory, data bricks, Azure Synapse, SQL DB, ADLS etc. Collaborate with business stakeholders to identify and meet data requirements. Design and implement solutions. Manage, monitor, and ensure the security and privacy of data to satisfy business needs. Design ingestion layer for structured & unstructured data & implement insurance specific data model for business & analytics use. Deliver ETL solution including data extraction, transformation, cleansing, data integration and data management. Implement batch & near real time data ingestion pipelines based on reference architecture. Ability to augment with new sources of dataRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Sc in Computers, B.Tech/B.E. in ComputersPG :MS/M.Sc(Science) in ComputersDoctorate :Doctorate Not RequiredKey SkillsData EngineeringAzure SynapseADLSData ManagementAzure Data factoryData bricksSQL DBETLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'Azure Synapse', 'ADLS', 'Data Management', 'Azure Data factory', 'Data bricks', 'SQL DB', 'ETL']","['UG :B.Sc in Computers, B.Tech/B.E. in Computers', 'PG :MS/M.Sc(Science) in Computers', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
A Product development Client of our primarily into pharmaceutical and Life Science Domain,"RoleBig Data Engineer,","₹ 15,00,000 - 27,50,000 P.A. ",5 - 10 years,Chennai,"Job descriptionRole : Big Data Developer(SE/SSE/Tech Lead/Architect)Job Location  Bangalore/ChennaiQualification Any GraduateExperience : 3 to 12 yearsJOB DESCRIPTIONWork with extremely talented peers in a client environment to build new and enhance/maintain existing codes that generates analytics insights leveraging the big data environment in AWS PySpark, PythonProficiency in SQL Writing, SQL Concepts, Data Modelling Techniques & Data Engineering ConceptsExperience in working with batch processing / real-time systems using various technologies like , Databricks, HDFS, Redshift, Hadoop,  AirflowElastic MapReduce on AWSFamiliar with tools like Git, Code Commit, Jenkins, Code PipelineWork in a Cross functional team along with other Data Engineers, QA Engineers and DevOps Engineers.Work with Functional Analysts to understand the core functionalities of the solution and build Data Models, Design DocumentsDevelop, test and implement data solutions based on finalized design documents.Ensure adherence with Security and Compliance policiesStay up-to-date with evolving cloud technologies and development best-practices including open source software.Work in an Agile EnvironmentRoleBig Data Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduateKey SkillsBig dataAzure Data FactoryPysparkHiveadfredshiftdata engineerHadoopMapreduceHdfsSkills highlighted with ‘‘ are preferred keyskills","['Big data', 'Azure Data Factory', 'Pyspark', 'Hive', 'adf', 'redshift', 'data engineer', 'Hadoop', 'Mapreduce', 'Hdfs']",['UG :Any Graduate'],"Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleDatabase Architect / Designer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job description     As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with Java & Spark programming.             Responsibilities:                Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.              Develop programs in Scala, Java and Python as part of data cleaning and processing             Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems using Core Java technology stack             Develop efficient software code for multiple use cases leveraging Core Java and Big Data technologies for various use cases built on the platform             Provide high operational excellence guaranteeing high availability and platform stability             Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.              If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here.         Required Technical and Professional Expertise            Minimum 4 years of experience in Big Data technologies             Minimum 4 years of experience in Java and multi-threading programming             Expertise in Python, Spark and Hadoop technologies             Proficient in development using SQL, Hive, Scala,              Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting             Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers          Preferred Technical and Professional Expertise            Expertise in Python or Scala programming              You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies             Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work             Intuitive individual with an ability to manage change and proven time management             Proven interpersonal skills while contributing to team effort by accomplishing related results as needed             Up-to-date technical knowledge by attending educational workshops, reviewing publications       RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsCloud computingbig dataUnix shell scriptingSQLPythonInterpersonal skillsHadoopSCALAProgrammingSkills highlighted with ‘‘ are preferred keyskills","['Cloud computing', 'big data', 'Unix shell scripting', 'SQL', 'Python', 'Interpersonal skills', 'Hadoop', 'SCALA', 'Programming']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
 https://collabera.com ,"RoleData Engineer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job descriptionQualifications:Bachelors Degree.5+ years of experience with 2+ years of strong experience working with data.Strong communication skills: ability to be well-organized, clear, and concise in conversations and written communications with development team members, business analysts, and project management.Proficiency in Data Engineering best practices.Experience working with Agile teams.Experience in Healthcare will be an added advantage.Good knowledge of SQL and NoSQL databases and are skilled in writing SQL queries to manipulate and retrieve data.Good knowledge and hands-on experience on Python, SQL, Scala, R programing language.Proficiency with AWS tools: S3, RDS, Good to have - Athena, Lake Formation, Glue, Redshift, Step Functions, CloudFormationProficiency in various data cleansing techniques, tools and ETL process.Good to have: experience with one or more business intelligence tools (Power BI, Quick Sight, Tableau) . Data visualization skills: custom visualization programming experience preferredRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :Any GraduateKey SkillsLake formationRedshift AwsAws GlueAthenapythonSkills highlighted with ‘‘ are preferred keyskills","['Lake formation', 'Redshift Aws', 'Aws Glue', 'Athena', 'python']",['UG :Any Graduate'],"Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleFull Stack Developer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job descriptionAs Senior Talend Developer, you will serve as a liaison among business partners, technical resources, and project stake holders to identify, articulate and facilitate business process and systems changes related to document digitization and document- driven business processes.      Your Role and Responsibilities           As Data engineer, you will develop and move data from the operational and external environments to the business intelligence environment using Ab Initio software. Skills include designing and developing extract, transform and load (ETL) processes.                Responsibilities:              Coordinate with multiple technical teams to ensure apt integration of functions to identify and define necessary system enhancements to deploy new products and process improvements           Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint           Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation           Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals           Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards           Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions,         Required Technical and Professional Expertise           Minimum 4 years of experience in ETL Datastage development           Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting           Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers         Preferred Technical and Professional Expertise           You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies           Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work           Intuitive individual with an ability to manage change and proven time management           Proven interpersonal skills while contributing to team effort by accomplishing related results as needed           Up-to-date technical knowledge by attending educational workshops, reviewing publications      RoleFull Stack Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsBusiness processCodingDatastageBusiness intelligenceUnix shell scriptingdata cleansingInterpersonal skillsTime managementCreative designingDebuggingSkills highlighted with ‘‘ are preferred keyskills","['Business process', 'Coding', 'Datastage', 'Business intelligence', 'Unix shell scripting', 'data cleansing', 'Interpersonal skills', 'Time management', 'Creative designing', 'Debugging']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Roppen Transportation Services Private Limited,"RoleData Engineer,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job descriptionRole and Responsibilities:Creating complex data processing pipelines, as part of diverse, high energy teamsDesigning scalable implementations of the models developed by our Data ScientistsBeing able to deploy models in real-time applications either as part of a microservice(HTTP or RPC) with bounded context or as realtime pipelines producing events in response to user actions on groundHands-on programming based on TDD, usually in a pair programming environmentDeploying data pipelines in production based on Continuous Delivery practices.Able to build and operate Data Pipelines, Build and operate Data Storage, Is familiar with Infrastructure definition and automation in this context. Is aware of adjacent technologies to the ones they have worked on. Good understanding of Data Modelling.Involve in building and deploying large scale data processing pipelines in a production environment.Experience building data pipelines and data centric applications using distributed storage solutions(including and not limited to HDFS like storage, Elasticsearch, Mongo, Kafka, Postgres/Mysql etc)Job RequirementTechnical Competencies:Experience in HDFS, S3, NoSql Databases and distributed platforms like Hadoop, Spark, Flink, Hive, Kafka, Oozie, Airflow, Elasticsearch etc.Experience in any of MapR, Cloudera, and HortonWorks and/ or cloud based Hadoop Distributions(GCP preferred).Experience creating and building data centric application involving ML modelsFunctional / Behavioural Competencies:Actively seeking to learn newer tech and curiously experimenting is a trait that would be preferred.Excellent understanding of technology landscapeLearning ability: Applies theoretical knowledge to practiceFocus on excellenceMentoring team matesEducation & Experiences:B. Tech, M. Tech (in Computer Sciences preferred)Around 3+ years of experience. For transitioned data engineers over all experience of 5+ years in preferred.Interview Process:Round 1 – AssignmentRound 2 – Technical Discussion 1Round 3 – Technical Discussion 2/ Managerial RoundRound 4 – HR RoundRoleData Engineer,Industry TypeCourier / Logistics,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in ComputersPG :M.Tech in ComputersKey SkillsData Engineeringhiveclouderasparkoozieairflowkafkadata processinghdfshadoopMLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'hive', 'cloudera', 'spark', 'oozie', 'airflow', 'kafka', 'data processing', 'hdfs', 'hadoop', 'ML']","['UG :B.Tech/B.E. in Computers', 'PG :M.Tech in Computers']","Industry TypeCourier / Logistics,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Dun & Bradstreet Technologies & Data Services is the global center of excellence for providing technology and analytics solutions and services to Dun & Bradstreet and its clients globally. We specialize in delivering predictive models and decision management solutions to convert raw data into actionable insights. Our customers include banks, financial services organizations, credit bureaus, rating agencies and thousands of businesses across the globe.Located in Chennai and Bangalore (India), the company employs high caliber Data Scientists, economists, analytics professionals and data engineers from some of the finest institutions in the country striving to create more transparent credit & business environment and robust economies. ","RoleData Engineer,",Not Disclosed,7 - 10 years,Chennai,"Job description  Data Engineers need to develop, construct, test and maintain data pipelines that allows data to move between systems.   Ingesting the data from its raw sources and processing it so that it can be used at different stages of a machine learning or data analytics project.Taking the data from multiple sources and coming up with right database design  that uses structure and unstructured dataMust be able to work from end to end. The candidate must be able  to write reusable and robust program/ETL pipeline/connector to process raw data (not in templates, un-unified formats, various sources, etc.), clean and extract target informationMust be able to design, optimize, and document data model and for resiliency, efficiency, and scalability. Trigger data loading based on event or  schedule to refresh database.Collaborate with IT teams and data scientist to devise a data strategy that addresses industry requirements.Build an inventory of data needed to implement the big data architecture for fast processing.Research new opportunities for data acquisition.Identify and evaluate current data management technologies.Create a fluid, end-to-end pipeline for how data will flow through different data science work stream.Develop data models for database structuresDesign, document, construct and deploy database architectures and applications (e.g. large relational databases and no-sql database)Integrate technical functionality (e.g. scalability, security, performance, data recovery, reliability, etc.)Implement measures to ensure data accuracy and accessibilityConstantly monitor, refine and report on the performance of data management systemsMeld new systems with existing warehouse structuresProduce and enforce database development standardsMaintain a repository of all data architecture artifacts and proceduresRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData PipelineData engineerData ModelingETLdatabase architectureDatabase structureSkills highlighted with ‘‘ are preferred keyskills","['Data Pipeline', 'Data engineer', 'Data Modeling', 'ETL', 'database architecture', 'Database structure']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
TransOrg Analytics is a Big Data and machine learning solutions and services company transforming businesses.,"RoleData Engineer,",Not Disclosed,3 - 5 years,Mumbai,"Job descriptionRoles and Responsibility1. Design and implement data engineering projects. 2. Integrate multiple data sources to create data lake/data mart Perform data ingestion and ETL processes using SQL, Scoop, Spark or Hive 3. Knowledge of new components and various emerging technologies in on-premises and Cloud (AWS/Azure/Google)4. Collaborate with various cross-functional teams: infrastructure, network and database5. Work with various teams to setup and manage users, secure and govern platforms and data and maintain business continuity through contingency plans (data archiving etc.)6. Monitor job performances, manage file system/disk-space, cluster & database connectivity, log files, manage backup/security and troubleshoot various user issues7. Design, implement, test and document performance benchmarking strategy for platforms as well as for different use cases8. Setup, administer, monitor, tune, optimize and govern large scale implementations9. Implement machine learning models on real time input data stream10. Drive customer communication during critical events and participate/lead various operational improvement initiativesDesired Candidate Profile :1. 3 - 5 years relevant experience in data engineering2. Exposure to any or all latest data engineering ecosystem platforms such as AWS, Azure, GCP, Cloudera and Data bricks3. Sound knowledge of Python/Scala/Java4. Good knowledge of SQL / NoSQL databases and data warehouse concepts5. Hands on experience of working on databases such as Sql Servers, PostgreSql, Cloud infrastructure, etc.6. Excellent knowledge of data backup, recovery, security and integrity7. Sound knowledge on Spark, HDFS/HIVE/HBASE, Shell Scripting, and Spark Streaming8. Excellent communication skills9. Must be proficient with data ingestion tools like Sqoop, flume, talend, and KafkaRoleData Engineer,Industry TypeAnalytics / KPO / Research,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey Skillsdata engineeringJavaCloud infrastructurePostgreSqlData ManagementData WarehousingData AnalyticsAnalyticsSQLPythonSql ServersSkills highlighted with ‘‘ are preferred keyskills","['data engineering', 'Java', 'Cloud infrastructure', 'PostgreSql', 'Data Management', 'Data Warehousing', 'Data Analytics', 'Analytics', 'SQL', 'Python', 'Sql Servers']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeAnalytics / KPO / Research,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleData Engineer,",Not Disclosed,7 - 9 years,Mumbai,"Job descriptionYour Role and ResponsibilitiesAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with python programming on AWS cloud platformResponsibilities:Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.Develop programs in Python as part of data cleaning and processingResponsible to design and develop distributed, high volume, high velocity multi-threaded event processing systemsDevelop efficient software code for multiple use cases leveraging Big Data technologies for various use cases built on the platformProvide high operational excellence guaranteeing high availability and platform stabilityImplement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.Required Technical and Professional ExpertiseMinimum 7 + years of experience in IT IndustryAt least 3 to 5years of development in Big Data and AWS Cloud (S3, Redshift, Glue, Lambda, Hadoop/EMR, Hive, Kinesis, Sqoop, Spark )Programming / Scripting in Python is a MUST.SQL, Data Warehouse skills are a MUST.Data engineering concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)Preferred Technical and Professional ExpertiseYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologiesAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to workIntuitive individual with an ability to manage change and proven time managementProven interpersonal skills while contributing to team effort by accomplishing related results as neededUp-to-date technical knowledge by attending educational workshops, reviewing publicationsRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Sc in Computers, B.Tech/B.E. in ComputersPG :MS/M.Sc(Science) in ComputersDoctorate :Doctorate Not RequiredKey SkillsData EngineeringAWS Data SyncHadoopBig DataSparkAWS DMSETLPythonSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'AWS Data Sync', 'Hadoop', 'Big Data', 'Spark', 'AWS DMS', 'ETL', 'Python']","['UG :B.Sc in Computers, B.Tech/B.E. in Computers', 'PG :MS/M.Sc(Science) in Computers', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
" OverviewPrivaini provides visibility into privacy risk and actionable insights to enterprises with a fact-based, systematic approach to mitigate reputation & legal risk for data privacy in their business network.  Our offerings start with a comprehensive Privacy Risk Score based on a companys privacy practices and historical events, deep web and dark web activities, and include features for a business to understand how the outside world looks at its privacy practices, perform competitive analysis, monitor 3rd party and 4th party privacy risk with change analysis, manage privacy policy reviews, privacy impact assessment and continuous monitoring of privacy changes for all services providers and business associates. We minimize asymmetric privacy information that businesses get from their 3rd party business partners.  Privaini is the largest repository of company privacy policies and practices – each policy is categorized, analyzed, and continuously monitored.  The product is designed for CPO/DPO, Risk management & Compliance officers, vendor management,  M&A groups, insurers, and teams that focus on data privacy risk management.  Websitehttp://www.privaini.com ","RoleData Scientist,","₹ 5,00,000 - 13,00,000 P.A. ",5 - 8 years,Bangalore/Bengaluru( Sadashiva Nagar ),"Job descriptionRoles and Responsibilities   PhD in Statistics, Math or Computer Science is preferred. Must have at least a Master degree with 10+ years of experience.Excellent statistical analysis skills to identify patterns in data. This includes having a keen sense of pattern detection and anomaly detection.ML Algorithm with High math background.Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Must be able to implement algorithms and statistical concepts to build predictive models.In-depth experience with common data science tools such as TensorFlow, PyTorch or equivalent.Proficient in programming with python, SQL and No-SQL databases; and data science libraries such as nltk, numpy, scipy and many othersGreat communication skills  both written and verbal. Must be able to effectively communicate with global English speaking teams.Expertise in collaborating with multi-disciplinary teams of business analysts, data scientists, subject matter experts, and developersDesired Candidate Profile Perks and Benefits RoleData Scientist,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Computers, B.Sc in Any SpecializationPG :M.Tech in Computers, MS/M.Sc(Science) in ComputersDoctorate :Ph.D/Doctorate in Computers, MathsKey SkillsTensorflowALGORITHMMachine LearningPytorchPythonScipySVMNumpySQLNltkAnomaly DetectionData Sciencek-nndecision forestsstatistical conceptsSkills highlighted with ‘‘ are preferred keyskills","['Tensorflow', 'ALGORITHM', 'Machine Learning', 'Pytorch', 'Python', 'Scipy', 'SVM', 'Numpy', 'SQL', 'Nltk', 'Anomaly Detection', 'Data Science', 'k-nn', 'decision forests', 'statistical concepts']","['UG :B.Tech/B.E. in Computers, B.Sc in Any Specialization', 'PG :M.Tech in Computers, MS/M.Sc(Science) in Computers', 'Doctorate :Ph.D/Doctorate in Computers, Maths']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Rolls-Royce India Private Ltd. Rolls-Royce started its journey in India over 80 years ago with the powering of the first Tata Aviation aircraft with Gypsy engines. We have continually expanded our footprint and are now a key player in the critical growth sectors of aerospace, marine, and energy with our integrated power systems. We have progressed from licensed production and engineering services to component manufacturing and supply chain activities. We're proud to have a strong presence in India, and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we are poised to become an engineering hub in the region. We support government's 'Make in India' initiative and are committed to strengthening our local footprint for high-end technology in the growing aerospace sector in India. We're searching for talented engineers to help us shape the future of Rolls-Royce in India.","RoleData Engineer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job description     An exciting opportunity has arisen for Data Engineer to join Rolls-Royce. You would be employed directly by Rolls-Royce Data Labs but would be based in the Bengaluru, Karnataka office.     Rolls-Royce is a world-leading provider of power systems and services, for use on land, at sea and in the air. We're proud to have a strong presence and an 80-year heritage in India and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we've been focused on the growth of the aerospace sector in India. Powering more than 50% of Wide Body Aircraft to and from India, we are poised to become an engineering hub in the region and are committed to growing our local footprint for high-end technology.     Rolls-Royce is one of the most technologically advanced organizations in the world - and our information systems are no exception. By improving information systems (applications and data) and technologies, we support overall business strategy and help teams throughout Rolls-Royce prepare for the future.      Key Accountabilities     Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.   Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).   Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.   Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.   Appreciate how to manage the data including; security, archiving, structure and storage.       Qualifications   and Skills     5+ years of experience at various levels of Software /Data Engineering roles   Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra   Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.   Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.   Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.      We offer excellent development, a competitive salary and exceptional benefits. These include bonus, employee support assistance and employee discounts.     Pioneer the performance of the future. Join us and you ll develop your skills and expertise to the very highest levels, working in an international environment for a company known the world over for brilliance and innovation. RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsSupply chainRDBMSInformaticaSSISPythonNoSQLAerospaceData processingMongoDBBusiness strategySkills highlighted with ‘‘ are preferred keyskills","['Supply chain', 'RDBMS', 'Informatica', 'SSIS', 'Python', 'NoSQL', 'Aerospace', 'Data processing', 'MongoDB', 'Business strategy']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Rolls Royce India Private Ltd.,"RoleData Engineer,",Not Disclosed,1 - 4 years,Bangalore/Bengaluru,"Job description           Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.    Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).    Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.    Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.    Appreciate how to manage the data including; security, archiving, structure and storage.        Qualifications    and Skills      5+ years of experience at various levels of Software /Data Engineering roles    Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra    Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.    Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.    Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.    RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsSupply chainNoSQLRDBMSAerospaceData processingMongoDBInformaticaBusiness strategySSISPython","['Supply chain', 'NoSQL', 'RDBMS', 'Aerospace', 'Data processing', 'MongoDB', 'Informatica', 'Business strategy', 'SSIS', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 674,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at accenture.com","RoleFull Stack Developer,",Not Disclosed,3 - 6 years,Bangalore/Bengaluru,"Job description Data Engineer-   3 years minimum experience with software engineering (Python), preferably in cloud implementations   Data engineer with proven experience (portfolio) setting up Big Data streaming pipelines using Apache Flink or other streaming technologies using time series data Proficiency with Kubernetes Experience setting up and managing Kubernetes clusters containing Flink jobs is a plus (configuration, memory management, log management, checkpointing, etc.) Coding proficiency in Python and Scala   Nice to Have: experience with the Azure Cloud solutions hands-on experience with Linux/UNIX based Systems experience working in agile / scrum development processes proven knowledge and mastery of DevOps tools and processes proven knowledge and mastery of big data challenges and tools specialization in implementation of state-of-the-art big data management and processing algorithms for practical business applications.RoleFull Stack Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsUnixLinuxCodingApachePythonData managementMemory managementConsultingSCALASkills highlighted with ‘‘ are preferred keyskills","['Unix', 'Linux', 'Coding', 'Apache', 'Python', 'Data management', 'Memory management', 'Consulting', 'SCALA']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleData Engineer,",Not Disclosed,2 - 4 years,Bangalore/Bengaluru,"Job description        Develop, test and support future-ready data solutions for customers across industry verticals       Develop, test and support end-to-end batch and near real-time data flows/pipelines       Demonstrate understanding in data architectures, modern data platforms, big data, ML/AI, analytics, cloud platforms, data governance and information management and associated technologies       Communicates risks and ensures understanding of these risks.       Required Technical and Professional Expertise         Minimum of 2 years of related experience required       Experience in modeling and business system designs       Good hands on experience on DataStage, Alteryx, Informatica & SSIS       Have great expertise in writing TSQL code       Experience in writing advanced Python and Spark for Data Processing       Well versed with data warehouse schemas and OLAP techniques       Preferred Technical and Professional Expertise         Ability to manage and make decisions about competing priorities and resources.       Ability to delegate where appropriate.       Must be a strong team player/leader.       Ability to lead Data transformation project with multiple junior data engineers        Strong oral written and interpersonal skills for interacting and throughout all levels of the organization.        Ability to clearly communicate complex business problems and technical solutions.    RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsData processingInformation managementInformaticaSSISAnalyticsInterpersonal skillsDatastagedata governanceOLAPPythonSkills highlighted with ‘‘ are preferred keyskills","['Data processing', 'Information management', 'Informatica', 'SSIS', 'Analytics', 'Interpersonal skills', 'Datastage', 'data governance', 'OLAP', 'Python']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleData warehouse Developer,",Not Disclosed,2 - 4 years,Bangalore/Bengaluru,"Job descriptionDevelop, test and support future-ready data solutions for customers across industry verticals Develop, test and support end-to-end batch and near real-time data flows/pipelines Demonstrate understanding in data architectures, modern data platforms, big data, ML/AI, analytics, cloud platforms, data governance and information management and associated technologies Communicates risks and ensures understanding of these risks. Required Technical and Professional Expertise Minimum of 2 years of related experience required Experience in modeling and business system designs Good hands on experience on DataStage, Alteryx, Informatica & SSIS Have great expertise in writing TSQL code Experience in writing advanced Python and Spark for Data Processing Well versed with data warehouse schemas and OLAP techniques Preferred Technical and Professional Expertise Ability to manage and make decisions about competing priorities and resources. Ability to delegate where appropriate. Must be a strong team player/leader. Ability to lead Data transformation project with multiple junior data engineers Strong oral written and interpersonal skills for interacting and throughout all levels of the organization. Ability to clearly communicate complex business problems and technical solutions. RoleData warehouse Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Post Graduation Not RequiredKey SkillsData processingInformation managementInformaticaSSISAnalyticsInterpersonal skillsDatastagedata governanceOLAPPythonSkills highlighted with ‘‘ are preferred keyskills","['Data processing', 'Information management', 'Informatica', 'SSIS', 'Analytics', 'Interpersonal skills', 'Datastage', 'data governance', 'OLAP', 'Python']","['UG :Any Graduate', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"Vedantu is the pioneer of LIVE Online Learning in India and has reddened online learning since. Our aim is to make quality learning accessible to students in all parts of the country, from the comfort of their homes. We have students from 1000+ Cities and 30+ Countries studying from teachers across 300+ cities ","RoleData Engineer,",Not Disclosed,4 - 9 years,Bangalore/Bengaluru,"Job description     Roles and Responsibilities  Data Platform Engineering at Vedantu builds distributed components, systems, and tools that power decisions at ‘Vedantu’. We have an incredibly rich dataset to collect, transform, and analyze in order to improve the effectiveness of our marketplace and create delight for our customers. The data scientists, machine learning engineers, tech engineers and analysts use this data to make the experience of using 'Vedantu' Platform better for the customers. We leverage existing open source technologies like Kafka, Hadoop, Hive, Presto/Trino, Data Lake and also write our own. As a member of our team you would spend time designing and growing our existing Data Warehouse, democratising data access at the company, and promoting the correct use of data and analytics at the company.   As an integral part of the Data Platform team, take ownership of multiple modules from      design to deployment.  Extensively build scalable, high-performance distributed systems that deal with large data  volumes.  Evaluating and understanding various data sources and data access techniques. Data sources include S3, No SQL databases, shared disk storage, Relational databases, Data Lake, Streaming Data etc. Understand the concepts and principles of query federation, data modeling and can produce, maintain and update relevant data models across multiple subject areas. Collaborate with different teams in order to understand / resolve data availability and consistency issues. Experience in designing metadata repositories, understanding a range of metadata tools and technologies to implement metadata repositories and working with metadata. ] Provide solution improvement and deployment best practices on the Data-Modeling platform. Responsible for deploying solutions following data industry standard compliances, regulation and guidelines. Desired Candidate Profile   Understanding of Big Data Engineering/processing, Business Intelligence and Advanced   analytics.  Knowledge in databases and Data Warehouse modeling, Solid understanding of SQL and  databases (Mysql, Postgres etc.), including SQL and NoSQL.  Experience in Trino (Presto) distributed SQL engine.  Technical expertise with data models, data pipeline, streaming analytics and advanced   analytics.  Expert in programming languages  KSQL, Spark SQL, Streaming/ Hive, Presto, Apache Iceberg and Data Lake.  Experience in accessing data from various data sources  AWS S3, Relational Databases, Data  Lakes, No SQL databases etc.  Knowledge of docker, Kubernetes, AWS EKS and other AWS services.  Be agile and ready take new challenges  Work in multi-functional groups with diverse interests and requirements to achieve a common goal. Perks and Benefits RoleData Engineer,Industry TypeE-Learning / EdTech,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E. in Any SpecializationKey SkillsData WarehousingStreamingflinkhiveData EngineeringKafkaData ModelingSparksenior data engineerSQLprestoSkills highlighted with ‘‘ are preferred keyskills","['Data Warehousing', 'Streaming', 'flink', 'hive', 'Data Engineering', 'Kafka', 'Data Modeling', 'Spark', 'senior data engineer', 'SQL', 'presto']",['UG :B.Tech/B.E. in Any Specialization'],"Industry TypeE-Learning / EdTech,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleData Engineer,",Not Disclosed,7 - 9 years,Hyderabad/Secunderabad,"Job descriptionUsing services and tools to ingest, egress, and transform data from multiple sources IN depth technical knowledge of tools like Azure Data factory, data bricks, Azure Synapse, SQL DB, ADLS etc. Collaborate with business stakeholders to identify and meet data requirements. Design and implement solutions. Manage, monitor, and ensure the security and privacy of data to satisfy business needs. Design ingestion layer for structured & unstructured data & implement insurance specific data model for business & analytics use. Deliver ETL solution including data extraction, transformation, cleansing, data integration and data management. Implement batch & near real time data ingestion pipelines based on reference architecture. Ability to augment with new sources of dataRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Sc in Computers, B.Tech/B.E. in ComputersPG :MS/M.Sc(Science) in ComputersDoctorate :Doctorate Not RequiredKey SkillsData EngineeringAzure SynapseADLSData ManagementAzure Data factoryData bricksSQL DBETLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'Azure Synapse', 'ADLS', 'Data Management', 'Azure Data factory', 'Data bricks', 'SQL DB', 'ETL']","['UG :B.Sc in Computers, B.Tech/B.E. in Computers', 'PG :MS/M.Sc(Science) in Computers', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
A Product development Client of our primarily into pharmaceutical and Life Science Domain,"RoleBig Data Engineer,","₹ 15,00,000 - 27,50,000 P.A. ",5 - 10 years,Chennai,"Job descriptionRole : Big Data Developer(SE/SSE/Tech Lead/Architect)Job Location  Bangalore/ChennaiQualification Any GraduateExperience : 3 to 12 yearsJOB DESCRIPTIONWork with extremely talented peers in a client environment to build new and enhance/maintain existing codes that generates analytics insights leveraging the big data environment in AWS PySpark, PythonProficiency in SQL Writing, SQL Concepts, Data Modelling Techniques & Data Engineering ConceptsExperience in working with batch processing / real-time systems using various technologies like , Databricks, HDFS, Redshift, Hadoop,  AirflowElastic MapReduce on AWSFamiliar with tools like Git, Code Commit, Jenkins, Code PipelineWork in a Cross functional team along with other Data Engineers, QA Engineers and DevOps Engineers.Work with Functional Analysts to understand the core functionalities of the solution and build Data Models, Design DocumentsDevelop, test and implement data solutions based on finalized design documents.Ensure adherence with Security and Compliance policiesStay up-to-date with evolving cloud technologies and development best-practices including open source software.Work in an Agile EnvironmentRoleBig Data Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduateKey SkillsBig dataAzure Data FactoryPysparkHiveadfredshiftdata engineerHadoopMapreduceHdfsSkills highlighted with ‘‘ are preferred keyskills","['Big data', 'Azure Data Factory', 'Pyspark', 'Hive', 'adf', 'redshift', 'data engineer', 'Hadoop', 'Mapreduce', 'Hdfs']",['UG :Any Graduate'],"Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleDatabase Architect / Designer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job description     As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with Java & Spark programming.             Responsibilities:                Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.              Develop programs in Scala, Java and Python as part of data cleaning and processing             Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems using Core Java technology stack             Develop efficient software code for multiple use cases leveraging Core Java and Big Data technologies for various use cases built on the platform             Provide high operational excellence guaranteeing high availability and platform stability             Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.              If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here.         Required Technical and Professional Expertise            Minimum 4 years of experience in Big Data technologies             Minimum 4 years of experience in Java and multi-threading programming             Expertise in Python, Spark and Hadoop technologies             Proficient in development using SQL, Hive, Scala,              Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting             Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers          Preferred Technical and Professional Expertise            Expertise in Python or Scala programming              You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies             Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work             Intuitive individual with an ability to manage change and proven time management             Proven interpersonal skills while contributing to team effort by accomplishing related results as needed             Up-to-date technical knowledge by attending educational workshops, reviewing publications       RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsCloud computingbig dataUnix shell scriptingSQLPythonInterpersonal skillsHadoopSCALAProgrammingSkills highlighted with ‘‘ are preferred keyskills","['Cloud computing', 'big data', 'Unix shell scripting', 'SQL', 'Python', 'Interpersonal skills', 'Hadoop', 'SCALA', 'Programming']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
 https://collabera.com ,"RoleData Engineer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job descriptionQualifications:Bachelors Degree.5+ years of experience with 2+ years of strong experience working with data.Strong communication skills: ability to be well-organized, clear, and concise in conversations and written communications with development team members, business analysts, and project management.Proficiency in Data Engineering best practices.Experience working with Agile teams.Experience in Healthcare will be an added advantage.Good knowledge of SQL and NoSQL databases and are skilled in writing SQL queries to manipulate and retrieve data.Good knowledge and hands-on experience on Python, SQL, Scala, R programing language.Proficiency with AWS tools: S3, RDS, Good to have - Athena, Lake Formation, Glue, Redshift, Step Functions, CloudFormationProficiency in various data cleansing techniques, tools and ETL process.Good to have: experience with one or more business intelligence tools (Power BI, Quick Sight, Tableau) . Data visualization skills: custom visualization programming experience preferredRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :Any GraduateKey SkillsLake formationRedshift AwsAws GlueAthenapythonSkills highlighted with ‘‘ are preferred keyskills","['Lake formation', 'Redshift Aws', 'Aws Glue', 'Athena', 'python']",['UG :Any Graduate'],"Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleFull Stack Developer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job descriptionAs Senior Talend Developer, you will serve as a liaison among business partners, technical resources, and project stake holders to identify, articulate and facilitate business process and systems changes related to document digitization and document- driven business processes.      Your Role and Responsibilities           As Data engineer, you will develop and move data from the operational and external environments to the business intelligence environment using Ab Initio software. Skills include designing and developing extract, transform and load (ETL) processes.                Responsibilities:              Coordinate with multiple technical teams to ensure apt integration of functions to identify and define necessary system enhancements to deploy new products and process improvements           Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint           Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation           Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals           Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards           Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions,         Required Technical and Professional Expertise           Minimum 4 years of experience in ETL Datastage development           Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting           Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers         Preferred Technical and Professional Expertise           You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies           Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work           Intuitive individual with an ability to manage change and proven time management           Proven interpersonal skills while contributing to team effort by accomplishing related results as needed           Up-to-date technical knowledge by attending educational workshops, reviewing publications      RoleFull Stack Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsBusiness processCodingDatastageBusiness intelligenceUnix shell scriptingdata cleansingInterpersonal skillsTime managementCreative designingDebuggingSkills highlighted with ‘‘ are preferred keyskills","['Business process', 'Coding', 'Datastage', 'Business intelligence', 'Unix shell scripting', 'data cleansing', 'Interpersonal skills', 'Time management', 'Creative designing', 'Debugging']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Roppen Transportation Services Private Limited,"RoleData Engineer,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job descriptionRole and Responsibilities:Creating complex data processing pipelines, as part of diverse, high energy teamsDesigning scalable implementations of the models developed by our Data ScientistsBeing able to deploy models in real-time applications either as part of a microservice(HTTP or RPC) with bounded context or as realtime pipelines producing events in response to user actions on groundHands-on programming based on TDD, usually in a pair programming environmentDeploying data pipelines in production based on Continuous Delivery practices.Able to build and operate Data Pipelines, Build and operate Data Storage, Is familiar with Infrastructure definition and automation in this context. Is aware of adjacent technologies to the ones they have worked on. Good understanding of Data Modelling.Involve in building and deploying large scale data processing pipelines in a production environment.Experience building data pipelines and data centric applications using distributed storage solutions(including and not limited to HDFS like storage, Elasticsearch, Mongo, Kafka, Postgres/Mysql etc)Job RequirementTechnical Competencies:Experience in HDFS, S3, NoSql Databases and distributed platforms like Hadoop, Spark, Flink, Hive, Kafka, Oozie, Airflow, Elasticsearch etc.Experience in any of MapR, Cloudera, and HortonWorks and/ or cloud based Hadoop Distributions(GCP preferred).Experience creating and building data centric application involving ML modelsFunctional / Behavioural Competencies:Actively seeking to learn newer tech and curiously experimenting is a trait that would be preferred.Excellent understanding of technology landscapeLearning ability: Applies theoretical knowledge to practiceFocus on excellenceMentoring team matesEducation & Experiences:B. Tech, M. Tech (in Computer Sciences preferred)Around 3+ years of experience. For transitioned data engineers over all experience of 5+ years in preferred.Interview Process:Round 1 – AssignmentRound 2 – Technical Discussion 1Round 3 – Technical Discussion 2/ Managerial RoundRound 4 – HR RoundRoleData Engineer,Industry TypeCourier / Logistics,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in ComputersPG :M.Tech in ComputersKey SkillsData Engineeringhiveclouderasparkoozieairflowkafkadata processinghdfshadoopMLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'hive', 'cloudera', 'spark', 'oozie', 'airflow', 'kafka', 'data processing', 'hdfs', 'hadoop', 'ML']","['UG :B.Tech/B.E. in Computers', 'PG :M.Tech in Computers']","Industry TypeCourier / Logistics,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Dun & Bradstreet Technologies & Data Services is the global center of excellence for providing technology and analytics solutions and services to Dun & Bradstreet and its clients globally. We specialize in delivering predictive models and decision management solutions to convert raw data into actionable insights. Our customers include banks, financial services organizations, credit bureaus, rating agencies and thousands of businesses across the globe.Located in Chennai and Bangalore (India), the company employs high caliber Data Scientists, economists, analytics professionals and data engineers from some of the finest institutions in the country striving to create more transparent credit & business environment and robust economies. ","RoleData Engineer,",Not Disclosed,7 - 10 years,Chennai,"Job description  Data Engineers need to develop, construct, test and maintain data pipelines that allows data to move between systems.   Ingesting the data from its raw sources and processing it so that it can be used at different stages of a machine learning or data analytics project.Taking the data from multiple sources and coming up with right database design  that uses structure and unstructured dataMust be able to work from end to end. The candidate must be able  to write reusable and robust program/ETL pipeline/connector to process raw data (not in templates, un-unified formats, various sources, etc.), clean and extract target informationMust be able to design, optimize, and document data model and for resiliency, efficiency, and scalability. Trigger data loading based on event or  schedule to refresh database.Collaborate with IT teams and data scientist to devise a data strategy that addresses industry requirements.Build an inventory of data needed to implement the big data architecture for fast processing.Research new opportunities for data acquisition.Identify and evaluate current data management technologies.Create a fluid, end-to-end pipeline for how data will flow through different data science work stream.Develop data models for database structuresDesign, document, construct and deploy database architectures and applications (e.g. large relational databases and no-sql database)Integrate technical functionality (e.g. scalability, security, performance, data recovery, reliability, etc.)Implement measures to ensure data accuracy and accessibilityConstantly monitor, refine and report on the performance of data management systemsMeld new systems with existing warehouse structuresProduce and enforce database development standardsMaintain a repository of all data architecture artifacts and proceduresRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData PipelineData engineerData ModelingETLdatabase architectureDatabase structureSkills highlighted with ‘‘ are preferred keyskills","['Data Pipeline', 'Data engineer', 'Data Modeling', 'ETL', 'database architecture', 'Database structure']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
TransOrg Analytics is a Big Data and machine learning solutions and services company transforming businesses.,"RoleData Engineer,",Not Disclosed,3 - 5 years,Mumbai,"Job descriptionRoles and Responsibility1. Design and implement data engineering projects. 2. Integrate multiple data sources to create data lake/data mart Perform data ingestion and ETL processes using SQL, Scoop, Spark or Hive 3. Knowledge of new components and various emerging technologies in on-premises and Cloud (AWS/Azure/Google)4. Collaborate with various cross-functional teams: infrastructure, network and database5. Work with various teams to setup and manage users, secure and govern platforms and data and maintain business continuity through contingency plans (data archiving etc.)6. Monitor job performances, manage file system/disk-space, cluster & database connectivity, log files, manage backup/security and troubleshoot various user issues7. Design, implement, test and document performance benchmarking strategy for platforms as well as for different use cases8. Setup, administer, monitor, tune, optimize and govern large scale implementations9. Implement machine learning models on real time input data stream10. Drive customer communication during critical events and participate/lead various operational improvement initiativesDesired Candidate Profile :1. 3 - 5 years relevant experience in data engineering2. Exposure to any or all latest data engineering ecosystem platforms such as AWS, Azure, GCP, Cloudera and Data bricks3. Sound knowledge of Python/Scala/Java4. Good knowledge of SQL / NoSQL databases and data warehouse concepts5. Hands on experience of working on databases such as Sql Servers, PostgreSql, Cloud infrastructure, etc.6. Excellent knowledge of data backup, recovery, security and integrity7. Sound knowledge on Spark, HDFS/HIVE/HBASE, Shell Scripting, and Spark Streaming8. Excellent communication skills9. Must be proficient with data ingestion tools like Sqoop, flume, talend, and KafkaRoleData Engineer,Industry TypeAnalytics / KPO / Research,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey Skillsdata engineeringJavaCloud infrastructurePostgreSqlData ManagementData WarehousingData AnalyticsAnalyticsSQLPythonSql ServersSkills highlighted with ‘‘ are preferred keyskills","['data engineering', 'Java', 'Cloud infrastructure', 'PostgreSql', 'Data Management', 'Data Warehousing', 'Data Analytics', 'Analytics', 'SQL', 'Python', 'Sql Servers']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeAnalytics / KPO / Research,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
KOCH BUSINESS SOLUTIONS INDIA PRIVATE LIMITED,"RoleData Engineer,",Not Disclosed,6 - 9 years,Bangalore/Bengaluru,"Job descriptionThis position will design, develop, enhance, debug, support, maintain and test AWS Cloud solutions that support the Georgia-Pacific business. These solutions may involve diverse platforms, software, technologies, and tools providing for a challenging and exciting environment. The position will be involved in all aspects of providing these solutions from design, development to implementation. The participant will not have direct reports but will be part of larger teams requiring excellent communication and collaboration skills with other technical groups as well as business leaders. Individuals should be able to work with minimal supervision and with general guidance as they deliver superior solutions to be used by the GP business.(job responsibilities)Working with Global functional and technical teams.Working with application architects to determine infrastructure requirements, solution design, developing and testing infrastructureCompleting key project work and support activitiesAdopting best practices in the implementation and execution of support processesChallenging the status quo and focusing on long-term value when designing solutionsUsing your technical and process aptitude to come up to speed on new tools and concepts required for integration development and supportSolving critical support issues related to infrastructure management tools and processesCollaborating with various IT teams such as infrastructure support, networking, database administrators, web development and business application developersEnabling business process efficiencies through strong application of AWS’ Well Architected FrameworkParticipate as infrastructure resource on small to large projects using both waterfall and agile methodologiesMaintaining system support documentationIdentifying and implementing process improvementsWhat You Will Need To Bring With You:(experience & education required)6 to 9 years IT experienceMinimum of 3 years hand on experience working with AWS IaaS and PaaS Compute, Storage, and Database services (S3, EC2, EBS, RDS, Lambda, IAM, RedShift, Glue, EMR).Hands on experience utilizing AWS Management Tools (CloudWatch, CloudTrail) to proactively monitor large and complex deploymentsBachelor’s degree in technology related fieldEconomic thinkingHands on experience working with users and architects to capture requirements, develop and test infrastructure.Strong verbal and written communicationStrong analytical and problem-solving skillMust be detailed-oriented.Ability to develop repeatable development and administration processes with supporting documentation.Experience with ETL & SQL scriptingWhat Will Put You Ahead:(experience & education preferred)AWS certificationExperience with SQL scripting, R and PythonExperience with RedshiftExperience with AWS ECS, EKS and other container orchestration technologiesExperience with MS SQL Server, Postgres, and MySQL database technologiesExperience with using cloud optimization tools to monitor spend and right-sizeExperience with ETL tools such as TalendOther Considerations:(physical demands/ unusual working conditions)Must be willing to carry a company-provided smart phone, have home high-speed Internet, andmay participate in a 24x7 on call rotationRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsData EngineeringS3PaaSDatabase servicesRedShiftREC2EBSMS SQL ServerPostgresTalendLambdaCloudTrailAWS IaaSPythonRDSMySQL databaseEMRSQL scriptingEKSRedshiftIAMGlueCloudWatchAWS ECSETLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'S3', 'PaaS', 'Database services', 'RedShift', 'R', 'EC2', 'EBS', 'MS SQL Server', 'Postgres', 'Talend', 'Lambda', 'CloudTrail', 'AWS IaaS', 'Python', 'RDS', 'MySQL database', 'EMR', 'SQL scripting', 'EKS', 'Redshift', 'IAM', 'Glue', 'CloudWatch', 'AWS ECS', 'ETL']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleData Engineer,",Not Disclosed,7 - 9 years,Mumbai,"Job descriptionYour Role and ResponsibilitiesAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with python programming on AWS cloud platformResponsibilities:Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.Develop programs in Python as part of data cleaning and processingResponsible to design and develop distributed, high volume, high velocity multi-threaded event processing systemsDevelop efficient software code for multiple use cases leveraging Big Data technologies for various use cases built on the platformProvide high operational excellence guaranteeing high availability and platform stabilityImplement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.Required Technical and Professional ExpertiseMinimum 7 + years of experience in IT IndustryAt least 3 to 5years of development in Big Data and AWS Cloud (S3, Redshift, Glue, Lambda, Hadoop/EMR, Hive, Kinesis, Sqoop, Spark )Programming / Scripting in Python is a MUST.SQL, Data Warehouse skills are a MUST.Data engineering concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)Preferred Technical and Professional ExpertiseYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologiesAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to workIntuitive individual with an ability to manage change and proven time managementProven interpersonal skills while contributing to team effort by accomplishing related results as neededUp-to-date technical knowledge by attending educational workshops, reviewing publicationsRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Sc in Computers, B.Tech/B.E. in ComputersPG :MS/M.Sc(Science) in ComputersDoctorate :Doctorate Not RequiredKey SkillsData EngineeringAWS Data SyncHadoopBig DataSparkAWS DMSETLPythonSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'AWS Data Sync', 'Hadoop', 'Big Data', 'Spark', 'AWS DMS', 'ETL', 'Python']","['UG :B.Sc in Computers, B.Tech/B.E. in Computers', 'PG :MS/M.Sc(Science) in Computers', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
" OverviewPrivaini provides visibility into privacy risk and actionable insights to enterprises with a fact-based, systematic approach to mitigate reputation & legal risk for data privacy in their business network.  Our offerings start with a comprehensive Privacy Risk Score based on a companys privacy practices and historical events, deep web and dark web activities, and include features for a business to understand how the outside world looks at its privacy practices, perform competitive analysis, monitor 3rd party and 4th party privacy risk with change analysis, manage privacy policy reviews, privacy impact assessment and continuous monitoring of privacy changes for all services providers and business associates. We minimize asymmetric privacy information that businesses get from their 3rd party business partners.  Privaini is the largest repository of company privacy policies and practices – each policy is categorized, analyzed, and continuously monitored.  The product is designed for CPO/DPO, Risk management & Compliance officers, vendor management,  M&A groups, insurers, and teams that focus on data privacy risk management.  Websitehttp://www.privaini.com ","RoleData Scientist,","₹ 5,00,000 - 13,00,000 P.A. ",5 - 8 years,Bangalore/Bengaluru( Sadashiva Nagar ),"Job descriptionRoles and Responsibilities   PhD in Statistics, Math or Computer Science is preferred. Must have at least a Master degree with 10+ years of experience.Excellent statistical analysis skills to identify patterns in data. This includes having a keen sense of pattern detection and anomaly detection.ML Algorithm with High math background.Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Must be able to implement algorithms and statistical concepts to build predictive models.In-depth experience with common data science tools such as TensorFlow, PyTorch or equivalent.Proficient in programming with python, SQL and No-SQL databases; and data science libraries such as nltk, numpy, scipy and many othersGreat communication skills  both written and verbal. Must be able to effectively communicate with global English speaking teams.Expertise in collaborating with multi-disciplinary teams of business analysts, data scientists, subject matter experts, and developersDesired Candidate Profile Perks and Benefits RoleData Scientist,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Computers, B.Sc in Any SpecializationPG :M.Tech in Computers, MS/M.Sc(Science) in ComputersDoctorate :Ph.D/Doctorate in Computers, MathsKey SkillsTensorflowALGORITHMMachine LearningPytorchPythonScipySVMNumpySQLNltkAnomaly DetectionData Sciencek-nndecision forestsstatistical conceptsSkills highlighted with ‘‘ are preferred keyskills","['Tensorflow', 'ALGORITHM', 'Machine Learning', 'Pytorch', 'Python', 'Scipy', 'SVM', 'Numpy', 'SQL', 'Nltk', 'Anomaly Detection', 'Data Science', 'k-nn', 'decision forests', 'statistical concepts']","['UG :B.Tech/B.E. in Computers, B.Sc in Any Specialization', 'PG :M.Tech in Computers, MS/M.Sc(Science) in Computers', 'Doctorate :Ph.D/Doctorate in Computers, Maths']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Rolls-Royce India Private Ltd. Rolls-Royce started its journey in India over 80 years ago with the powering of the first Tata Aviation aircraft with Gypsy engines. We have continually expanded our footprint and are now a key player in the critical growth sectors of aerospace, marine, and energy with our integrated power systems. We have progressed from licensed production and engineering services to component manufacturing and supply chain activities. We're proud to have a strong presence in India, and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we are poised to become an engineering hub in the region. We support government's 'Make in India' initiative and are committed to strengthening our local footprint for high-end technology in the growing aerospace sector in India. We're searching for talented engineers to help us shape the future of Rolls-Royce in India.","RoleData Engineer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job description     An exciting opportunity has arisen for Data Engineer to join Rolls-Royce. You would be employed directly by Rolls-Royce Data Labs but would be based in the Bengaluru, Karnataka office.     Rolls-Royce is a world-leading provider of power systems and services, for use on land, at sea and in the air. We're proud to have a strong presence and an 80-year heritage in India and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we've been focused on the growth of the aerospace sector in India. Powering more than 50% of Wide Body Aircraft to and from India, we are poised to become an engineering hub in the region and are committed to growing our local footprint for high-end technology.     Rolls-Royce is one of the most technologically advanced organizations in the world - and our information systems are no exception. By improving information systems (applications and data) and technologies, we support overall business strategy and help teams throughout Rolls-Royce prepare for the future.      Key Accountabilities     Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.   Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).   Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.   Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.   Appreciate how to manage the data including; security, archiving, structure and storage.       Qualifications   and Skills     5+ years of experience at various levels of Software /Data Engineering roles   Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra   Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.   Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.   Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.      We offer excellent development, a competitive salary and exceptional benefits. These include bonus, employee support assistance and employee discounts.     Pioneer the performance of the future. Join us and you ll develop your skills and expertise to the very highest levels, working in an international environment for a company known the world over for brilliance and innovation. RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsSupply chainRDBMSInformaticaSSISPythonNoSQLAerospaceData processingMongoDBBusiness strategySkills highlighted with ‘‘ are preferred keyskills","['Supply chain', 'RDBMS', 'Informatica', 'SSIS', 'Python', 'NoSQL', 'Aerospace', 'Data processing', 'MongoDB', 'Business strategy']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Rolls Royce India Private Ltd.,"RoleData Engineer,",Not Disclosed,1 - 4 years,Bangalore/Bengaluru,"Job description           Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.    Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).    Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.    Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.    Appreciate how to manage the data including; security, archiving, structure and storage.        Qualifications    and Skills      5+ years of experience at various levels of Software /Data Engineering roles    Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra    Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.    Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.    Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.    RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsSupply chainNoSQLRDBMSAerospaceData processingMongoDBInformaticaBusiness strategySSISPython","['Supply chain', 'NoSQL', 'RDBMS', 'Aerospace', 'Data processing', 'MongoDB', 'Informatica', 'Business strategy', 'SSIS', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 674,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at accenture.com","RoleFull Stack Developer,",Not Disclosed,3 - 6 years,Bangalore/Bengaluru,"Job description Data Engineer-   3 years minimum experience with software engineering (Python), preferably in cloud implementations   Data engineer with proven experience (portfolio) setting up Big Data streaming pipelines using Apache Flink or other streaming technologies using time series data Proficiency with Kubernetes Experience setting up and managing Kubernetes clusters containing Flink jobs is a plus (configuration, memory management, log management, checkpointing, etc.) Coding proficiency in Python and Scala   Nice to Have: experience with the Azure Cloud solutions hands-on experience with Linux/UNIX based Systems experience working in agile / scrum development processes proven knowledge and mastery of DevOps tools and processes proven knowledge and mastery of big data challenges and tools specialization in implementation of state-of-the-art big data management and processing algorithms for practical business applications.RoleFull Stack Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsUnixLinuxCodingApachePythonData managementMemory managementConsultingSCALASkills highlighted with ‘‘ are preferred keyskills","['Unix', 'Linux', 'Coding', 'Apache', 'Python', 'Data management', 'Memory management', 'Consulting', 'SCALA']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleData Engineer,",Not Disclosed,2 - 4 years,Bangalore/Bengaluru,"Job description        Develop, test and support future-ready data solutions for customers across industry verticals       Develop, test and support end-to-end batch and near real-time data flows/pipelines       Demonstrate understanding in data architectures, modern data platforms, big data, ML/AI, analytics, cloud platforms, data governance and information management and associated technologies       Communicates risks and ensures understanding of these risks.       Required Technical and Professional Expertise         Minimum of 2 years of related experience required       Experience in modeling and business system designs       Good hands on experience on DataStage, Alteryx, Informatica & SSIS       Have great expertise in writing TSQL code       Experience in writing advanced Python and Spark for Data Processing       Well versed with data warehouse schemas and OLAP techniques       Preferred Technical and Professional Expertise         Ability to manage and make decisions about competing priorities and resources.       Ability to delegate where appropriate.       Must be a strong team player/leader.       Ability to lead Data transformation project with multiple junior data engineers        Strong oral written and interpersonal skills for interacting and throughout all levels of the organization.        Ability to clearly communicate complex business problems and technical solutions.    RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsData processingInformation managementInformaticaSSISAnalyticsInterpersonal skillsDatastagedata governanceOLAPPythonSkills highlighted with ‘‘ are preferred keyskills","['Data processing', 'Information management', 'Informatica', 'SSIS', 'Analytics', 'Interpersonal skills', 'Datastage', 'data governance', 'OLAP', 'Python']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleData warehouse Developer,",Not Disclosed,2 - 4 years,Bangalore/Bengaluru,"Job descriptionDevelop, test and support future-ready data solutions for customers across industry verticals Develop, test and support end-to-end batch and near real-time data flows/pipelines Demonstrate understanding in data architectures, modern data platforms, big data, ML/AI, analytics, cloud platforms, data governance and information management and associated technologies Communicates risks and ensures understanding of these risks. Required Technical and Professional Expertise Minimum of 2 years of related experience required Experience in modeling and business system designs Good hands on experience on DataStage, Alteryx, Informatica & SSIS Have great expertise in writing TSQL code Experience in writing advanced Python and Spark for Data Processing Well versed with data warehouse schemas and OLAP techniques Preferred Technical and Professional Expertise Ability to manage and make decisions about competing priorities and resources. Ability to delegate where appropriate. Must be a strong team player/leader. Ability to lead Data transformation project with multiple junior data engineers Strong oral written and interpersonal skills for interacting and throughout all levels of the organization. Ability to clearly communicate complex business problems and technical solutions. RoleData warehouse Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Post Graduation Not RequiredKey SkillsData processingInformation managementInformaticaSSISAnalyticsInterpersonal skillsDatastagedata governanceOLAPPythonSkills highlighted with ‘‘ are preferred keyskills","['Data processing', 'Information management', 'Informatica', 'SSIS', 'Analytics', 'Interpersonal skills', 'Datastage', 'data governance', 'OLAP', 'Python']","['UG :Any Graduate', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
