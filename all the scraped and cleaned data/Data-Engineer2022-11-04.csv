company,role,salary,experience,Location,description,skills,qualification,industry_type,Functional_area,Employment_type,Role_category
Leading Client,"RoleData Engineer,",Not Disclosed,7 - 9 years,Mumbai,"Job descriptionYour Role and ResponsibilitiesAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with python programming on AWS cloud platformResponsibilities:Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.Develop programs in Python as part of data cleaning and processingResponsible to design and develop distributed, high volume, high velocity multi-threaded event processing systemsDevelop efficient software code for multiple use cases leveraging Big Data technologies for various use cases built on the platformProvide high operational excellence guaranteeing high availability and platform stabilityImplement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.Required Technical and Professional ExpertiseMinimum 7 + years of experience in IT IndustryAt least 3 to 5years of development in Big Data and AWS Cloud (S3, Redshift, Glue, Lambda, Hadoop/EMR, Hive, Kinesis, Sqoop, Spark )Programming / Scripting in Python is a MUST.SQL, Data Warehouse skills are a MUST.Data engineering concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)Preferred Technical and Professional ExpertiseYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologiesAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to workIntuitive individual with an ability to manage change and proven time managementProven interpersonal skills while contributing to team effort by accomplishing related results as neededUp-to-date technical knowledge by attending educational workshops, reviewing publicationsRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E. in Computers, B.Sc in ComputersPG :MS/M.Sc(Science) in ComputersDoctorate :Doctorate Not RequiredKey SkillsData EngineeringAWS Data SyncHadoopBig DataSparkAWS DMSETLPythonSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'AWS Data Sync', 'Hadoop', 'Big Data', 'Spark', 'AWS DMS', 'ETL', 'Python']","['UG :B.Tech/B.E. in Computers, B.Sc in Computers', 'PG :MS/M.Sc(Science) in Computers', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
" OverviewPrivaini provides visibility into privacy risk and actionable insights to enterprises with a fact-based, systematic approach to mitigate reputation & legal risk for data privacy in their business network.  Our offerings start with a comprehensive Privacy Risk Score based on a companys privacy practices and historical events, deep web and dark web activities, and include features for a business to understand how the outside world looks at its privacy practices, perform competitive analysis, monitor 3rd party and 4th party privacy risk with change analysis, manage privacy policy reviews, privacy impact assessment and continuous monitoring of privacy changes for all services providers and business associates. We minimize asymmetric privacy information that businesses get from their 3rd party business partners.  Privaini is the largest repository of company privacy policies and practices – each policy is categorized, analyzed, and continuously monitored.  The product is designed for CPO/DPO, Risk management & Compliance officers, vendor management,  M&A groups, insurers, and teams that focus on data privacy risk management.  Websitehttp://www.privaini.com ","RoleData Scientist,","₹ 5,00,000 - 13,00,000 P.A. ",5 - 8 years,Bangalore/Bengaluru( Sadashiva Nagar ),"Job descriptionRoles and Responsibilities   PhD in Statistics, Math or Computer Science is preferred. Must have at least a Master degree with 10+ years of experience.Excellent statistical analysis skills to identify patterns in data. This includes having a keen sense of pattern detection and anomaly detection.ML Algorithm with High math background.Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Must be able to implement algorithms and statistical concepts to build predictive models.In-depth experience with common data science tools such as TensorFlow, PyTorch or equivalent.Proficient in programming with python, SQL and No-SQL databases; and data science libraries such as nltk, numpy, scipy and many othersGreat communication skills  both written and verbal. Must be able to effectively communicate with global English speaking teams.Expertise in collaborating with multi-disciplinary teams of business analysts, data scientists, subject matter experts, and developersDesired Candidate Profile Perks and Benefits RoleData Scientist,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Sc in Any Specialization, B.Tech/B.E. in ComputersPG :M.Tech in Computers, MS/M.Sc(Science) in ComputersDoctorate :Ph.D/Doctorate in Computers, MathsKey SkillsTensorflowALGORITHMMachine LearningPytorchPythonScipySVMNumpySQLNltkAnomaly DetectionData Sciencek-nndecision forestsstatistical conceptsSkills highlighted with ‘‘ are preferred keyskills","['Tensorflow', 'ALGORITHM', 'Machine Learning', 'Pytorch', 'Python', 'Scipy', 'SVM', 'Numpy', 'SQL', 'Nltk', 'Anomaly Detection', 'Data Science', 'k-nn', 'decision forests', 'statistical concepts']","['UG :B.Sc in Any Specialization, B.Tech/B.E. in Computers', 'PG :M.Tech in Computers, MS/M.Sc(Science) in Computers', 'Doctorate :Ph.D/Doctorate in Computers, Maths']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Rolls-Royce India Private Ltd. Rolls-Royce started its journey in India over 80 years ago with the powering of the first Tata Aviation aircraft with Gypsy engines. We have continually expanded our footprint and are now a key player in the critical growth sectors of aerospace, marine, and energy with our integrated power systems. We have progressed from licensed production and engineering services to component manufacturing and supply chain activities. We're proud to have a strong presence in India, and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we are poised to become an engineering hub in the region. We support government's 'Make in India' initiative and are committed to strengthening our local footprint for high-end technology in the growing aerospace sector in India. We're searching for talented engineers to help us shape the future of Rolls-Royce in India.","RoleData Engineer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job description     An exciting opportunity has arisen for Data Engineer to join Rolls-Royce. You would be employed directly by Rolls-Royce Data Labs but would be based in the Bengaluru, Karnataka office.     Rolls-Royce is a world-leading provider of power systems and services, for use on land, at sea and in the air. We're proud to have a strong presence and an 80-year heritage in India and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we've been focused on the growth of the aerospace sector in India. Powering more than 50% of Wide Body Aircraft to and from India, we are poised to become an engineering hub in the region and are committed to growing our local footprint for high-end technology.     Rolls-Royce is one of the most technologically advanced organizations in the world - and our information systems are no exception. By improving information systems (applications and data) and technologies, we support overall business strategy and help teams throughout Rolls-Royce prepare for the future.      Key Accountabilities     Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.   Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).   Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.   Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.   Appreciate how to manage the data including; security, archiving, structure and storage.       Qualifications   and Skills     5+ years of experience at various levels of Software /Data Engineering roles   Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra   Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.   Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.   Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.      We offer excellent development, a competitive salary and exceptional benefits. These include bonus, employee support assistance and employee discounts.     Pioneer the performance of the future. Join us and you ll develop your skills and expertise to the very highest levels, working in an international environment for a company known the world over for brilliance and innovation. RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsSupply chainRDBMSInformaticaSSISPythonNoSQLAerospaceData processingMongoDBBusiness strategySkills highlighted with ‘‘ are preferred keyskills","['Supply chain', 'RDBMS', 'Informatica', 'SSIS', 'Python', 'NoSQL', 'Aerospace', 'Data processing', 'MongoDB', 'Business strategy']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Rolls Royce India Private Ltd.,"RoleData Engineer,",Not Disclosed,1 - 4 years,Bangalore/Bengaluru,"Job description           Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.    Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).    Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.    Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.    Appreciate how to manage the data including; security, archiving, structure and storage.        Qualifications    and Skills      5+ years of experience at various levels of Software /Data Engineering roles    Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra    Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.    Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.    Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.    RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsSupply chainNoSQLRDBMSAerospaceData processingMongoDBInformaticaBusiness strategySSISPython","['Supply chain', 'NoSQL', 'RDBMS', 'Aerospace', 'Data processing', 'MongoDB', 'Informatica', 'Business strategy', 'SSIS', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"GlobalLogic is an American digital services company providing software product design and development services. It is an independent subsidiary of Hitachi Ltd. GlobalLogic has corporate headquarters in San Jose, California. ","RoleData Engineer,",Not Disclosed,3 - 7 years,Bangalore/Bengaluru,"Job descriptionJob Description: Data Engineer Experience• 5+ years of experience in data engineering and data managementSensing tool for predictive hiring. Role includes developing data models, creating data pipelines,and managing data retrieval, storage and distribution.• Hadoop, Hive, Pig, Scala (Good to have), Java• Amazon Web Services/Redshift (for data warehousing)• Strong Algorithms, Data Cleansing techniques along with ETL Education Qualification  • Bachelors in Computer Science or Math or Statistics RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Any SpecializationPG :M.Tech in Any Specialization, MS/M.Sc(Science) in Any SpecializationKey SkillsHadoopAWSSQLHiveAzurejavaetldata engineeringSkills highlighted with ‘‘ are preferred keyskills","['Hadoop', 'AWS', 'SQL', 'Hive', 'Azure', 'java', 'etl', 'data engineering']","['UG :B.Tech/B.E. in Any Specialization', 'PG :M.Tech in Any Specialization, MS/M.Sc(Science) in Any Specialization']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleDatabase Architect / Designer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job description     As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with Java & Spark programming.             Responsibilities:                Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.              Develop programs in Scala, Java and Python as part of data cleaning and processing             Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems using Core Java technology stack             Develop efficient software code for multiple use cases leveraging Core Java and Big Data technologies for various use cases built on the platform             Provide high operational excellence guaranteeing high availability and platform stability             Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.              If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here.         Required Technical and Professional Expertise            Minimum 4 years of experience in Big Data technologies             Minimum 4 years of experience in Java and multi-threading programming             Expertise in Python, Spark and Hadoop technologies             Proficient in development using SQL, Hive, Scala,              Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting             Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers          Preferred Technical and Professional Expertise            Expertise in Python or Scala programming              You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies             Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work             Intuitive individual with an ability to manage change and proven time management             Proven interpersonal skills while contributing to team effort by accomplishing related results as needed             Up-to-date technical knowledge by attending educational workshops, reviewing publications       RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsCloud computingbig dataUnix shell scriptingSQLPythonInterpersonal skillsHadoopSCALAProgrammingSkills highlighted with ‘‘ are preferred keyskills","['Cloud computing', 'big data', 'Unix shell scripting', 'SQL', 'Python', 'Interpersonal skills', 'Hadoop', 'SCALA', 'Programming']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"Founded in 1976, CGI is among the largest IT and business consulting services firms in the world. Operating in hundreds of locations across the globe, CGI delivers end-to-end services and solutions, including strategic IT and business consulting, systems integration, intellectual property, and managed IT and business process services","RoleData Engineer,",Not Disclosed,3 - 6 years,Bangalore/Bengaluru,"Job description   Design and develop ETL processes based on functional and non-functional requirements in Azure platform  Design and develop migration process from SQL Server to Azure platform  Understand the full end to end development activities from design to go live for ETL development in Azure platform  Recommend and execute improvements  Document component design for developers and for broader communication.  Understand and adopt an Agile (SCRUM like) software development mindset  Follow established processes/standards, business technology architecture for development, release management and deployment process  Execute and provide support during testing cycles and post-production deployment, engage in peer code reviews.   Job Requirement for Azure ETL Data Engineer:   Must-have skills  Undergraduate Degree or Technical Certificate.  Microsoft Azure certified - Data Engineer   Working experience (> 3 years) in Azure Data Factory, Azure Databricks, ADLS, Datadog, Azure SQL DB, Azure Synapse   Hands-on experience in Python / Spark / Pyspark / JSON  SQL Server (SSIS)  Working experience with data modeling, relational modeling and dimensional modeling  General knowledge about file formats (e.g. XML, CSV, JSON), databases (e.g. MS SQL, Oracle) and different type of connectivity is also very useful.  Data files movement via mailbox  Source-code versioning/promotion tools, e.g. Git/Jenkins  Orchestration tools in Azure platform   Nice-to-have skills  Sqoop / Hadoop / Shell Script  Java / Scala  Experience in Agile environment  Tracking and collaboration tools (e.g. JIRA, Confluence, MS Teams)      Skills:    Azure architect   Azure Data Factory   Azure Data Lake   Data Engineering   Azure SQL Data Warehouse   PowerShell   PowerShell   Python   Shell Script           RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsData modelingXMLSSISSQLPythonCGIJSONJIRARelease managementSkills highlighted with ‘‘ are preferred keyskills","['Data modeling', 'XML', 'SSIS', 'SQL', 'Python', 'CGI', 'JSON', 'JIRA', 'Release management']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Marlabs designs and develops advanced digital solutions that help our clients improve their business outcomes swiftly and precisely. We succeed by harnessing the power of the Digital Collective, which brings together design-led digital innovation with human experience, composable digital platforms, and our collaborative ecosystem of first-class technology partners and innovators. We provide digital-first strategy and advisory services, digital labs for rapid solution incubation and prototyping, and agile engineering to build and scale digital solutions, as well as prize-winning products and platforms in AI and analytics, cybersecurity, and IoT. Our offerings help leading companies around the world make operations sleeker, keep customers closer, transform data into decisions, de-risk cyberspace, boost legacy system performance and seize novel opportunities and new digital revenue streams.","RoleData Engineer,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job description      Passionate about intuitive data models and an expert in distributed data processing patterns.          Highly proficient in SQL as well as the ability to help others build and tune their SQL statements.          Comfortable with at least one of C#, Python or Scala.          Experienced in developing and deploying data pipelines using SSIS and/or Azure Data Factory.          Understand the Data Lifecycle and concepts such as lineage, governance, privacy, retention, anonymity, etc.            What you will be doing            Gather and translate business requirements into technical specifications.          You will design and develop data pipelines and reporting solutions in a hands-on manner.          Assist with building tabular models and reports on them using DAX.          Help in improving data quality and record-keeping procedures to ensure the highest levels of data integrity          Research and implement best practices to continuously optimize existing processes          Help Starr modernize our hybrid technology solutions including the opportunity to work on modern warehousing and integration technologies.          Collaborate with reporting and engineering teams to define technology roadmap and architectural designs to improve our data platform scalability.          Become a domain expert within Starrs data stores and support the build of high-quality analytics as an architect and an individual contributor.          Engage in troubleshooting production issues and root cause analysis.            Preferred Qualifications            At least 5 years of experience in software engineering and data management.          At least 3 years of experience with developing data/application integrations.          3 years of experience working with SQL Server          At least 1-2 years of experience of working on Power BI/DAX          REST or similar API development experience          Experience on Cloud technologies with Azure          Snowflake or Redshift Data warehouse implementation experience.       RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsRoot cause analysisData managementpower biData qualityBusiness intelligenceTroubleshootingSSISData warehousingSQLPython","['Root cause analysis', 'Data management', 'power bi', 'Data quality', 'Business intelligence', 'Troubleshooting', 'SSIS', 'Data warehousing', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
GROUPON SHARED SERVICES PVT LTD,"RoleData Engineer,",Not Disclosed,5 - 8 years,Bangalore/Bengaluru,"Job descriptionYou’ll spend time on the following: You will take ownership of the technical aspects of implementing data pipeline & migration requirements, ensuring that the platform is being used to its fullest potential through designing and building applications around our customer’s needs. Migrate on-premise data applications & pipelines to GCP cloud leveraging technologies such as Terraform, Spark, Airflow etc. Work directly with our internal product/technical teams to ensure that our technology infrastructure is seamlessly and effectively integrated with our third-party software, conceive, and build the necessary applications to make this happen. Provide on-call support for migration-related issues wherever applicable. Interface directly with stakeholders to gather requirements and own the automated end-to-end data engineering solutions. Implement data pipelines to automate the ingestion, transformation, and augmentation of both structured and unstructured data sources, and provide best practices for pipeline operations Troubleshoot and remediate data quality issues raised by pipeline alerts or downstream consumers. Provide advice and ideas for technical solutions and improvements to data systems Create and maintain clear documentation on data models/schemas as well as transformation/validation rules Implement tools that help data consumers to extract, analyze, and visualize data faster through data pipelines Leading the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for batch ETL's. Engage with stakeholders to gather requirements to deliver data solutions We’re excited if you have: Bachelor’s degree in Computer Science or equivalent with substantial data engineering experience. 5+ years of recent hands-on experience with a modern programming language (Scala, Python, Java) is required; Spark/Pyspark is preferred. Experience with version control apps (ie: GitHub) and experience working within a CI/CD framework is a plus. An even bigger plus if you have experience building framework 5+ years of recent hands-on SQL programming experience in a Big Data environment is required; Hadoop/Hive experience is preferred. Experience with GCP Cloud, Teradata Vantage is a plus Experience developing and maintaining ETL applications and data pipelines using big data technologies is required; Airflow experience is a plus! Experience in managing multiple projects and stakeholders with excellent communication and interpersonal skills Experience building data solutions for visualization software's (ie: Tableau) is a plus Ability to develop and organize high-quality documentation Superior analytical skills and a strong sense of ownership in your work Ability to thrive in a fast-paced start-up environment, and to manage multiple, competing priorities simultaneously Prior e-commerce experience is a big plus. We value engineers who are: Customer-focused: We believe that doing what’s right for the customer is ultimately what will drive our business forward. Obsessed with quality: Your production code just works & scales linearly Team players. You believe that more can be achieved together. You listen to feedback and also provide supportive feedback to help others grow/improve. Fast learners: We are willing to disrupt our existing business to trial new products and solutions. You love learning how to use new technologies and then rapidly apply them to new problems. Pragmatic: We do things quickly to learn what our customers desire. You know when it’s appropriate to take shortcuts that don’t sacrifice quality or maintainability. Owners: Engineers at Groupon know how to positively impact the business. RoleData Engineer,Industry TypeEvents / Live Entertainment,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :BCA in Computers, B.Sc in Computers, B.Tech/B.E. in ComputersPG :Any PostgraduateKey SkillsData EngineeringJavaPysparkHiveGitHubHadoopBig DataTableauTeradataPythonSQLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'Java', 'Pyspark', 'Hive', 'GitHub', 'Hadoop', 'Big Data', 'Tableau', 'Teradata', 'Python', 'SQL']","['UG :BCA in Computers, B.Sc in Computers, B.Tech/B.E. in Computers', 'PG :Any Postgraduate']","Industry TypeEvents / Live Entertainment,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleData Engineer,",Not Disclosed,3 - 5 years,Bangalore/Bengaluru,"Job description- 2+ years of development experience in at least one of MySQL, Oracle, PostgreSQL or MSSQL and with Big Data frameworks / platforms / data stores like Apache Drill, Arrow, Hadoop, HDFS, Spark, MapR etc- Strong experience setting up data warehouses, data modeling, data wrangling and dataflow architecture on the cloud- Strong experience in 3+ experience with public cloud services such as AWS, Azure, or GCP and languages like Java/ Python etc- 2+ years of development experience in Amazon Redshift, Google Bigquery or Azure data warehouse platforms preferred- Knowledge of statistical analysis tools like R, SAS etc- Familiarity with any data visualization software- A growth mindset and passionate about building things from the ground up and most importantly, you should be fun to work withAs a data engineer at, you will:- Create and maintain optimal data pipeline architecture,- Assemble large, complex data sets that meet functional / non-functional business requirements.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS - big data- technologies.- Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.- Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.- Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.- Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Sc in Any Specialization, B.Tech/B.E. in Any Specialization, BCA in Any SpecializationPG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsData EngineeringMapRdata modelingPostgreSQLMySQLHadoopBig DataHDFSSparkSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'MapR', 'data modeling', 'PostgreSQL', 'MySQL', 'Hadoop', 'Big Data', 'HDFS', 'Spark']","['UG :B.Sc in Any Specialization, B.Tech/B.E. in Any Specialization, BCA in Any Specialization', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleDatabase Architect / Designer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job descriptionR- BIG QUERY+ BIG DATA/ANALYTICS/PYSPARKJob Responsibilities :- Use different data warehousing concepts to build a data warehouse for reporting purpose- Assist Analytics/ Reporting teams in setting up data pipelines & monitoring daily jobs- Lead and provide guidance to junior members in team. Oversee project life cycle from intake through delivery- Responsible for Planning, Architecture and Design for journey towards Spark, Hadoop Big Data and cloud solutions like AWS, Databricks, etc- Develop and test ETL components to high standards of data quality and act as hands-on development leadMust have :- 4+ years of experience in ETL (or) data engineering role in an analytics environment- Bachelor's degree from Tier I colleges (Comp. Sci degree preferred not mandatory)- Expertise in building data pipelines in Big data platforms; Good understanding of Data warehousing concepts- Knowledge of Shell scripting, SQL, Python & some of the standard data science packages (Pandas, Numpy, etc.)- Prior Exposure- to Big Data Technologies (Spark, Hadoop, Hive & so on)- Exposure to ETL software like Talend, Informatica, etc. is a plus; Not Mandatory- Strong verbal and business communication skills- Strong business acumen & demonstrated an aptitude for analytics that incite actionRoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :BCA in Computers, B.Tech/B.E. in Computers, B.Sc in ComputersPG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsData SciencePysparkHiveBig Data AnalyticsData EngineeringHadoopSparkNumpyPythonSQLSkills highlighted with ‘‘ are preferred keyskills","['Data Science', 'Pyspark', 'Hive', 'Big Data Analytics', 'Data Engineering', 'Hadoop', 'Spark', 'Numpy', 'Python', 'SQL']","['UG :BCA in Computers, B.Tech/B.E. in Computers, B.Sc in Computers', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
Leading Client,"RoleDatabase Architect / Designer,",Not Disclosed,8 - 12 years,Hyderabad/Secunderabad,"Job descriptionRole and responsibilities :- Creating Project Technical Documentation- Designing Solution architecture, and work on Data Ingestion, Preparation and Transformation. Debugging the production failures and identifying the solution.- Developing efficient frameworks for development and testing using (AWS Dynamo DB, EKS, Kafka, KinesisSparkStreamingPython) to enable seamless data ingestion process on to the Hadoop platform.- Enabling Data Governance and Data Discovery on Hadoop Platform- Building data processing framework using Spark, HQL- Exposure of Security Framework with Kerberos, Ranger, Atlas- Exposure of Data Pipeline Automation using DevOps tools- Exposure of Job Monitoring framework along validations automation- Exposure of handling structured, Un Structured and Streaming dataTechnical skills requirements :- The candidate must demonstrate proficiency in,- Solid hands-on and Solution Architecting experience in Big-Data Technologies (AWS preferred)- Hands on experience in: AWS Dynamo DB, EKS, Kafka, Kinesis, Glue PySpark, EMR PySpark- Hands-on experience of programming language like Python, Scala with Spark.- Good command and working experience on HadoopMap Reduce, HDFS, Hive, HBase, and No-SQL Databases- Hands on working experience on any of the data engineeringanalytics platform (HortonworksCloudera MapR AWS), AWS preferred- Hands-on experience on Data Ingestion Apache Nifi, Apache Airflow, Sqoop, and Oozie- Hands on working experience of data processing at scale with event driven systems, message queues (Kafka FlinkSpark Streaming)- Hands on working Experience with AWS Services like EMR, Kinesis, S3, CloudFormation, Glue, API Gateway, Lake Foundation- Hands on working Experience with AWS Athena- Data Warehouse exposure on Apache Nifi, Apache Airflow, Kylo- Operationalization of ML models on AWS (e.g. deployment, scheduling, model monitoring etc.)- Feature EngineeringData Processing to be used for Model development- Experience gathering and processing raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.)- Experience building data pipelines for structuredunstructured, real-timebatch, eventssynchronous asynchronous using MQ, Kafka, Steam processing- Hands-on working experience in analyzing source system data and data flows, working with structured and unstructured data- Must be very strong in writing SQL queries- Strengthen the Data engineering team with Big Data solutions- Strong technical, analytical, and problem-solving skills- Strong organizational skills, with the ability to work autonomously as well as in a team-based environment- Pleasant Personality, Strong Communication & Interpersonal SkillsRoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsKafkaData AnalyticsData IngestionMQData PipelineCloudFormationBig DataAWSData GovernanceSolution ArchitectWeb ScrapingSkills highlighted with ‘‘ are preferred keyskills","['Kafka', 'Data Analytics', 'Data Ingestion', 'MQ', 'Data Pipeline', 'CloudFormation', 'Big Data', 'AWS', 'Data Governance', 'Solution Architect', 'Web Scraping']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleFull Stack Developer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job descriptionAs Senior Talend Developer, you will serve as a liaison among business partners, technical resources, and project stake holders to identify, articulate and facilitate business process and systems changes related to document digitization and document- driven business processes.      Your Role and Responsibilities           As Data engineer, you will develop and move data from the operational and external environments to the business intelligence environment using Ab Initio software. Skills include designing and developing extract, transform and load (ETL) processes.                Responsibilities:              Coordinate with multiple technical teams to ensure apt integration of functions to identify and define necessary system enhancements to deploy new products and process improvements           Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint           Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation           Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals           Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards           Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions,         Required Technical and Professional Expertise           Minimum 4 years of experience in ETL Datastage development           Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting           Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers         Preferred Technical and Professional Expertise           You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies           Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work           Intuitive individual with an ability to manage change and proven time management           Proven interpersonal skills while contributing to team effort by accomplishing related results as needed           Up-to-date technical knowledge by attending educational workshops, reviewing publications      RoleFull Stack Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsBusiness processCodingDatastageBusiness intelligenceUnix shell scriptingdata cleansingInterpersonal skillsTime managementCreative designingDebuggingSkills highlighted with ‘‘ are preferred keyskills","['Business process', 'Coding', 'Datastage', 'Business intelligence', 'Unix shell scripting', 'data cleansing', 'Interpersonal skills', 'Time management', 'Creative designing', 'Debugging']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Roppen Transportation Services Private Limited,"RoleData Engineer,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job descriptionRole and Responsibilities:Creating complex data processing pipelines, as part of diverse, high energy teamsDesigning scalable implementations of the models developed by our Data ScientistsBeing able to deploy models in real-time applications either as part of a microservice(HTTP or RPC) with bounded context or as realtime pipelines producing events in response to user actions on groundHands-on programming based on TDD, usually in a pair programming environmentDeploying data pipelines in production based on Continuous Delivery practices.Able to build and operate Data Pipelines, Build and operate Data Storage, Is familiar with Infrastructure definition and automation in this context. Is aware of adjacent technologies to the ones they have worked on. Good understanding of Data Modelling.Involve in building and deploying large scale data processing pipelines in a production environment.Experience building data pipelines and data centric applications using distributed storage solutions(including and not limited to HDFS like storage, Elasticsearch, Mongo, Kafka, Postgres/Mysql etc)Job RequirementTechnical Competencies:Experience in HDFS, S3, NoSql Databases and distributed platforms like Hadoop, Spark, Flink, Hive, Kafka, Oozie, Airflow, Elasticsearch etc.Experience in any of MapR, Cloudera, and HortonWorks and/ or cloud based Hadoop Distributions(GCP preferred).Experience creating and building data centric application involving ML modelsFunctional / Behavioural Competencies:Actively seeking to learn newer tech and curiously experimenting is a trait that would be preferred.Excellent understanding of technology landscapeLearning ability: Applies theoretical knowledge to practiceFocus on excellenceMentoring team matesEducation & Experiences:B. Tech, M. Tech (in Computer Sciences preferred)Around 3+ years of experience. For transitioned data engineers over all experience of 5+ years in preferred.Interview Process:Round 1 – AssignmentRound 2 – Technical Discussion 1Round 3 – Technical Discussion 2/ Managerial RoundRound 4 – HR RoundRoleData Engineer,Industry TypeCourier / Logistics,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in ComputersPG :M.Tech in ComputersKey SkillsData Engineeringhiveclouderasparkoozieairflowkafkadata processinghdfshadoopMLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'hive', 'cloudera', 'spark', 'oozie', 'airflow', 'kafka', 'data processing', 'hdfs', 'hadoop', 'ML']","['UG :B.Tech/B.E. in Computers', 'PG :M.Tech in Computers']","Industry TypeCourier / Logistics,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
" ""We are a set of technocratic people who believe that the data is in our DNA and impacts every little decision we make in the modern world today. The biggest first and foremost impact of data is how we define money and consumption of money across the financial sector. With this conviction, we are currently focused vertically on the BFSI & Fintech domain. Horizontally, we are working on the most cutting-edge data technology ecosystem and helping our clients build products for the future of how the whole human lives consume disruptive technology and its benefits. In the pursuit of building this inevitable, we are always on the lookout to hire absolutely great next-gen talent who is not afraid of taking risks to make themselves relevant for the future. As an organization we have created a platform for young and dynamic engineering professionals to make mistakes, explore new technologies, and make themselves better with each passing day, while  working on some of the most complex yet thrilling business problem statements, to solve real-world problems."" ","RoleData Engineer,",Not Disclosed,2 - 6 years,Pune,"Job description Position: Data EngineerPackage: Best in class(Depending on the skillsets, experience & fitment)_______________________________________________________________________________________________People who are serving their notice period or have served their notice period and who can demonstrate joining date in the next***** 10 to 12 *****days are only expected to apply for this job_______________________________________________________________________________________________Core Responsibilities : The candidate is expected to lead one of the key analytics area end to end. This is a pure hands on role. Ensure the solutions built meet the required best practices and coding standards. Ability to adapt any new technology if situation demands. Requirement gathering with business and get this prioritized in the sprint cycle. Should be able to take end to end responsibility of assigned task Ensure quality and timely delivery.Preference and Experience: Strong at PySpark, Python, Java fundamentals Good understanding of Data Structure Good at SQL query/optimization Strong fundamental of OOPs programming Good understanding of AWS Cloud,Big Data. Nice to have Data Lake,AWS Glue, Athena, S3, Kinesis, SQL/NoSQL DBAcademic qualifications : Must be a Technical Graduate  Btech / Mtech  Tier 1/2 colleges.Experience Range: 2 to 6 yearRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Any SpecializationPG :M.Tech in Any SpecializationKey SkillsPysparkData MigrationData PipelineAWSSQLData StructuresData AnalysisSparkETLPythonData IntegrationSkills highlighted with ‘‘ are preferred keyskills","['Pyspark', 'Data Migration', 'Data Pipeline', 'AWS', 'SQL', 'Data Structures', 'Data Analysis', 'Spark', 'ETL', 'Python', 'Data Integration']","['UG :B.Tech/B.E. in Any Specialization', 'PG :M.Tech in Any Specialization']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
Leading Client,"RoleData Engineer,",Not Disclosed,7 - 9 years,Mumbai,"Job descriptionYour Role and ResponsibilitiesAs Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with python programming on AWS cloud platformResponsibilities:Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.Develop programs in Python as part of data cleaning and processingResponsible to design and develop distributed, high volume, high velocity multi-threaded event processing systemsDevelop efficient software code for multiple use cases leveraging Big Data technologies for various use cases built on the platformProvide high operational excellence guaranteeing high availability and platform stabilityImplement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.Required Technical and Professional ExpertiseMinimum 7 + years of experience in IT IndustryAt least 3 to 5years of development in Big Data and AWS Cloud (S3, Redshift, Glue, Lambda, Hadoop/EMR, Hive, Kinesis, Sqoop, Spark )Programming / Scripting in Python is a MUST.SQL, Data Warehouse skills are a MUST.Data engineering concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)Preferred Technical and Professional ExpertiseYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologiesAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to workIntuitive individual with an ability to manage change and proven time managementProven interpersonal skills while contributing to team effort by accomplishing related results as neededUp-to-date technical knowledge by attending educational workshops, reviewing publicationsRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E. in Computers, B.Sc in ComputersPG :MS/M.Sc(Science) in ComputersDoctorate :Doctorate Not RequiredKey SkillsData EngineeringAWS Data SyncHadoopBig DataSparkAWS DMSETLPythonSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'AWS Data Sync', 'Hadoop', 'Big Data', 'Spark', 'AWS DMS', 'ETL', 'Python']","['UG :B.Tech/B.E. in Computers, B.Sc in Computers', 'PG :MS/M.Sc(Science) in Computers', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
" OverviewPrivaini provides visibility into privacy risk and actionable insights to enterprises with a fact-based, systematic approach to mitigate reputation & legal risk for data privacy in their business network.  Our offerings start with a comprehensive Privacy Risk Score based on a companys privacy practices and historical events, deep web and dark web activities, and include features for a business to understand how the outside world looks at its privacy practices, perform competitive analysis, monitor 3rd party and 4th party privacy risk with change analysis, manage privacy policy reviews, privacy impact assessment and continuous monitoring of privacy changes for all services providers and business associates. We minimize asymmetric privacy information that businesses get from their 3rd party business partners.  Privaini is the largest repository of company privacy policies and practices – each policy is categorized, analyzed, and continuously monitored.  The product is designed for CPO/DPO, Risk management & Compliance officers, vendor management,  M&A groups, insurers, and teams that focus on data privacy risk management.  Websitehttp://www.privaini.com ","RoleData Scientist,","₹ 5,00,000 - 13,00,000 P.A. ",5 - 8 years,Bangalore/Bengaluru( Sadashiva Nagar ),"Job descriptionRoles and Responsibilities   PhD in Statistics, Math or Computer Science is preferred. Must have at least a Master degree with 10+ years of experience.Excellent statistical analysis skills to identify patterns in data. This includes having a keen sense of pattern detection and anomaly detection.ML Algorithm with High math background.Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Must be able to implement algorithms and statistical concepts to build predictive models.In-depth experience with common data science tools such as TensorFlow, PyTorch or equivalent.Proficient in programming with python, SQL and No-SQL databases; and data science libraries such as nltk, numpy, scipy and many othersGreat communication skills  both written and verbal. Must be able to effectively communicate with global English speaking teams.Expertise in collaborating with multi-disciplinary teams of business analysts, data scientists, subject matter experts, and developersDesired Candidate Profile Perks and Benefits RoleData Scientist,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Sc in Any Specialization, B.Tech/B.E. in ComputersPG :M.Tech in Computers, MS/M.Sc(Science) in ComputersDoctorate :Ph.D/Doctorate in Computers, MathsKey SkillsTensorflowALGORITHMMachine LearningPytorchPythonScipySVMNumpySQLNltkAnomaly DetectionData Sciencek-nndecision forestsstatistical conceptsSkills highlighted with ‘‘ are preferred keyskills","['Tensorflow', 'ALGORITHM', 'Machine Learning', 'Pytorch', 'Python', 'Scipy', 'SVM', 'Numpy', 'SQL', 'Nltk', 'Anomaly Detection', 'Data Science', 'k-nn', 'decision forests', 'statistical concepts']","['UG :B.Sc in Any Specialization, B.Tech/B.E. in Computers', 'PG :M.Tech in Computers, MS/M.Sc(Science) in Computers', 'Doctorate :Ph.D/Doctorate in Computers, Maths']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"Rolls-Royce India Private Ltd. Rolls-Royce started its journey in India over 80 years ago with the powering of the first Tata Aviation aircraft with Gypsy engines. We have continually expanded our footprint and are now a key player in the critical growth sectors of aerospace, marine, and energy with our integrated power systems. We have progressed from licensed production and engineering services to component manufacturing and supply chain activities. We're proud to have a strong presence in India, and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we are poised to become an engineering hub in the region. We support government's 'Make in India' initiative and are committed to strengthening our local footprint for high-end technology in the growing aerospace sector in India. We're searching for talented engineers to help us shape the future of Rolls-Royce in India.","RoleData Engineer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job description     An exciting opportunity has arisen for Data Engineer to join Rolls-Royce. You would be employed directly by Rolls-Royce Data Labs but would be based in the Bengaluru, Karnataka office.     Rolls-Royce is a world-leading provider of power systems and services, for use on land, at sea and in the air. We're proud to have a strong presence and an 80-year heritage in India and are excited about our growing future in Bengaluru. Through innovative solutions and diverse, globally renowned products, we've been focused on the growth of the aerospace sector in India. Powering more than 50% of Wide Body Aircraft to and from India, we are poised to become an engineering hub in the region and are committed to growing our local footprint for high-end technology.     Rolls-Royce is one of the most technologically advanced organizations in the world - and our information systems are no exception. By improving information systems (applications and data) and technologies, we support overall business strategy and help teams throughout Rolls-Royce prepare for the future.      Key Accountabilities     Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.   Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).   Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.   Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.   Appreciate how to manage the data including; security, archiving, structure and storage.       Qualifications   and Skills     5+ years of experience at various levels of Software /Data Engineering roles   Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra   Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.   Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.   Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.      We offer excellent development, a competitive salary and exceptional benefits. These include bonus, employee support assistance and employee discounts.     Pioneer the performance of the future. Join us and you ll develop your skills and expertise to the very highest levels, working in an international environment for a company known the world over for brilliance and innovation. RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsSupply chainRDBMSInformaticaSSISPythonNoSQLAerospaceData processingMongoDBBusiness strategySkills highlighted with ‘‘ are preferred keyskills","['Supply chain', 'RDBMS', 'Informatica', 'SSIS', 'Python', 'NoSQL', 'Aerospace', 'Data processing', 'MongoDB', 'Business strategy']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Rolls Royce India Private Ltd.,"RoleData Engineer,",Not Disclosed,1 - 4 years,Bangalore/Bengaluru,"Job description           Securing the data supply chain, understanding how data is ingested from different sources and combined / transformed into a single data set. Understanding how to analyse, cleanse, join and transform data.    Implementing designed / specified solutions into the chosen platform (e.g. Azure Data Factories / Data Lakes, HDInsight, Talend, MuleSoft or traditional software).    Working with colleagues to ensure that the infrastructure available is capable of meeting the solution requirements.    Planning, designing and conducting tests of the implementations, correcting errors and re-testing to achieve an acceptable result.    Appreciate how to manage the data including; security, archiving, structure and storage.        Qualifications    and Skills      5+ years of experience at various levels of Software /Data Engineering roles    Experience in designing solutions using databases and data storage technology such as RDBMS, NoSQL, MongoDB, Hadoop, Cassandra    Experience building and optimizing Big Data data pipelines, architectures and data sets. Python experience is must.    Be up to date with data processing technology / platforms such as Spark (Databricks, Hortonworks, and Cloudera etc.), PowerBI, and Tableau.    Experience with ETL and/or data integration tools such as Informatica, SSIS, Talend, MuleSoft, Dell Boomi.    RoleData Engineer,Industry TypePower,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsSupply chainNoSQLRDBMSAerospaceData processingMongoDBInformaticaBusiness strategySSISPython","['Supply chain', 'NoSQL', 'RDBMS', 'Aerospace', 'Data processing', 'MongoDB', 'Informatica', 'Business strategy', 'SSIS', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypePower,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"GlobalLogic is an American digital services company providing software product design and development services. It is an independent subsidiary of Hitachi Ltd. GlobalLogic has corporate headquarters in San Jose, California. ","RoleData Engineer,",Not Disclosed,3 - 7 years,Bangalore/Bengaluru,"Job descriptionJob Description: Data Engineer Experience• 5+ years of experience in data engineering and data managementSensing tool for predictive hiring. Role includes developing data models, creating data pipelines,and managing data retrieval, storage and distribution.• Hadoop, Hive, Pig, Scala (Good to have), Java• Amazon Web Services/Redshift (for data warehousing)• Strong Algorithms, Data Cleansing techniques along with ETL Education Qualification  • Bachelors in Computer Science or Math or Statistics RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Any SpecializationPG :M.Tech in Any Specialization, MS/M.Sc(Science) in Any SpecializationKey SkillsHadoopAWSSQLHiveAzurejavaetldata engineeringSkills highlighted with ‘‘ are preferred keyskills","['Hadoop', 'AWS', 'SQL', 'Hive', 'Azure', 'java', 'etl', 'data engineering']","['UG :B.Tech/B.E. in Any Specialization', 'PG :M.Tech in Any Specialization, MS/M.Sc(Science) in Any Specialization']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleDatabase Architect / Designer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job description     As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the design of data solutions using Hadoop based technologies along with Java & Spark programming.             Responsibilities:                Responsible to Ingest data from files, streams and databases. Process the data with Hive, Hadoop, Spark.              Develop programs in Scala, Java and Python as part of data cleaning and processing             Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems using Core Java technology stack             Develop efficient software code for multiple use cases leveraging Core Java and Big Data technologies for various use cases built on the platform             Provide high operational excellence guaranteeing high availability and platform stability             Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Hadoop, any Cloud computing etc.              If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here.         Required Technical and Professional Expertise            Minimum 4 years of experience in Big Data technologies             Minimum 4 years of experience in Java and multi-threading programming             Expertise in Python, Spark and Hadoop technologies             Proficient in development using SQL, Hive, Scala,              Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting             Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers          Preferred Technical and Professional Expertise            Expertise in Python or Scala programming              You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies             Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work             Intuitive individual with an ability to manage change and proven time management             Proven interpersonal skills while contributing to team effort by accomplishing related results as needed             Up-to-date technical knowledge by attending educational workshops, reviewing publications       RoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsCloud computingbig dataUnix shell scriptingSQLPythonInterpersonal skillsHadoopSCALAProgrammingSkills highlighted with ‘‘ are preferred keyskills","['Cloud computing', 'big data', 'Unix shell scripting', 'SQL', 'Python', 'Interpersonal skills', 'Hadoop', 'SCALA', 'Programming']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"Founded in 1976, CGI is among the largest IT and business consulting services firms in the world. Operating in hundreds of locations across the globe, CGI delivers end-to-end services and solutions, including strategic IT and business consulting, systems integration, intellectual property, and managed IT and business process services","RoleData Engineer,",Not Disclosed,3 - 6 years,Bangalore/Bengaluru,"Job description   Design and develop ETL processes based on functional and non-functional requirements in Azure platform  Design and develop migration process from SQL Server to Azure platform  Understand the full end to end development activities from design to go live for ETL development in Azure platform  Recommend and execute improvements  Document component design for developers and for broader communication.  Understand and adopt an Agile (SCRUM like) software development mindset  Follow established processes/standards, business technology architecture for development, release management and deployment process  Execute and provide support during testing cycles and post-production deployment, engage in peer code reviews.   Job Requirement for Azure ETL Data Engineer:   Must-have skills  Undergraduate Degree or Technical Certificate.  Microsoft Azure certified - Data Engineer   Working experience (> 3 years) in Azure Data Factory, Azure Databricks, ADLS, Datadog, Azure SQL DB, Azure Synapse   Hands-on experience in Python / Spark / Pyspark / JSON  SQL Server (SSIS)  Working experience with data modeling, relational modeling and dimensional modeling  General knowledge about file formats (e.g. XML, CSV, JSON), databases (e.g. MS SQL, Oracle) and different type of connectivity is also very useful.  Data files movement via mailbox  Source-code versioning/promotion tools, e.g. Git/Jenkins  Orchestration tools in Azure platform   Nice-to-have skills  Sqoop / Hadoop / Shell Script  Java / Scala  Experience in Agile environment  Tracking and collaboration tools (e.g. JIRA, Confluence, MS Teams)      Skills:    Azure architect   Azure Data Factory   Azure Data Lake   Data Engineering   Azure SQL Data Warehouse   PowerShell   PowerShell   Python   Shell Script           RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsData modelingXMLSSISSQLPythonCGIJSONJIRARelease managementSkills highlighted with ‘‘ are preferred keyskills","['Data modeling', 'XML', 'SSIS', 'SQL', 'Python', 'CGI', 'JSON', 'JIRA', 'Release management']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
"Marlabs designs and develops advanced digital solutions that help our clients improve their business outcomes swiftly and precisely. We succeed by harnessing the power of the Digital Collective, which brings together design-led digital innovation with human experience, composable digital platforms, and our collaborative ecosystem of first-class technology partners and innovators. We provide digital-first strategy and advisory services, digital labs for rapid solution incubation and prototyping, and agile engineering to build and scale digital solutions, as well as prize-winning products and platforms in AI and analytics, cybersecurity, and IoT. Our offerings help leading companies around the world make operations sleeker, keep customers closer, transform data into decisions, de-risk cyberspace, boost legacy system performance and seize novel opportunities and new digital revenue streams.","RoleData Engineer,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job description      Passionate about intuitive data models and an expert in distributed data processing patterns.          Highly proficient in SQL as well as the ability to help others build and tune their SQL statements.          Comfortable with at least one of C#, Python or Scala.          Experienced in developing and deploying data pipelines using SSIS and/or Azure Data Factory.          Understand the Data Lifecycle and concepts such as lineage, governance, privacy, retention, anonymity, etc.            What you will be doing            Gather and translate business requirements into technical specifications.          You will design and develop data pipelines and reporting solutions in a hands-on manner.          Assist with building tabular models and reports on them using DAX.          Help in improving data quality and record-keeping procedures to ensure the highest levels of data integrity          Research and implement best practices to continuously optimize existing processes          Help Starr modernize our hybrid technology solutions including the opportunity to work on modern warehousing and integration technologies.          Collaborate with reporting and engineering teams to define technology roadmap and architectural designs to improve our data platform scalability.          Become a domain expert within Starrs data stores and support the build of high-quality analytics as an architect and an individual contributor.          Engage in troubleshooting production issues and root cause analysis.            Preferred Qualifications            At least 5 years of experience in software engineering and data management.          At least 3 years of experience with developing data/application integrations.          3 years of experience working with SQL Server          At least 1-2 years of experience of working on Power BI/DAX          REST or similar API development experience          Experience on Cloud technologies with Azure          Snowflake or Redshift Data warehouse implementation experience.       RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :Any GraduatePG :Any PostgraduateKey SkillsRoot cause analysisData managementpower biData qualityBusiness intelligenceTroubleshootingSSISData warehousingSQLPython","['Root cause analysis', 'Data management', 'power bi', 'Data quality', 'Business intelligence', 'Troubleshooting', 'SSIS', 'Data warehousing', 'SQL', 'Python']","['UG :Any Graduate', 'PG :Any Postgraduate']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
GROUPON SHARED SERVICES PVT LTD,"RoleData Engineer,",Not Disclosed,5 - 8 years,Bangalore/Bengaluru,"Job descriptionYou’ll spend time on the following: You will take ownership of the technical aspects of implementing data pipeline & migration requirements, ensuring that the platform is being used to its fullest potential through designing and building applications around our customer’s needs. Migrate on-premise data applications & pipelines to GCP cloud leveraging technologies such as Terraform, Spark, Airflow etc. Work directly with our internal product/technical teams to ensure that our technology infrastructure is seamlessly and effectively integrated with our third-party software, conceive, and build the necessary applications to make this happen. Provide on-call support for migration-related issues wherever applicable. Interface directly with stakeholders to gather requirements and own the automated end-to-end data engineering solutions. Implement data pipelines to automate the ingestion, transformation, and augmentation of both structured and unstructured data sources, and provide best practices for pipeline operations Troubleshoot and remediate data quality issues raised by pipeline alerts or downstream consumers. Provide advice and ideas for technical solutions and improvements to data systems Create and maintain clear documentation on data models/schemas as well as transformation/validation rules Implement tools that help data consumers to extract, analyze, and visualize data faster through data pipelines Leading the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for batch ETL's. Engage with stakeholders to gather requirements to deliver data solutions We’re excited if you have: Bachelor’s degree in Computer Science or equivalent with substantial data engineering experience. 5+ years of recent hands-on experience with a modern programming language (Scala, Python, Java) is required; Spark/Pyspark is preferred. Experience with version control apps (ie: GitHub) and experience working within a CI/CD framework is a plus. An even bigger plus if you have experience building framework 5+ years of recent hands-on SQL programming experience in a Big Data environment is required; Hadoop/Hive experience is preferred. Experience with GCP Cloud, Teradata Vantage is a plus Experience developing and maintaining ETL applications and data pipelines using big data technologies is required; Airflow experience is a plus! Experience in managing multiple projects and stakeholders with excellent communication and interpersonal skills Experience building data solutions for visualization software's (ie: Tableau) is a plus Ability to develop and organize high-quality documentation Superior analytical skills and a strong sense of ownership in your work Ability to thrive in a fast-paced start-up environment, and to manage multiple, competing priorities simultaneously Prior e-commerce experience is a big plus. We value engineers who are: Customer-focused: We believe that doing what’s right for the customer is ultimately what will drive our business forward. Obsessed with quality: Your production code just works & scales linearly Team players. You believe that more can be achieved together. You listen to feedback and also provide supportive feedback to help others grow/improve. Fast learners: We are willing to disrupt our existing business to trial new products and solutions. You love learning how to use new technologies and then rapidly apply them to new problems. Pragmatic: We do things quickly to learn what our customers desire. You know when it’s appropriate to take shortcuts that don’t sacrifice quality or maintainability. Owners: Engineers at Groupon know how to positively impact the business. RoleData Engineer,Industry TypeEvents / Live Entertainment,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :BCA in Computers, B.Sc in Computers, B.Tech/B.E. in ComputersPG :Any PostgraduateKey SkillsData EngineeringJavaPysparkHiveGitHubHadoopBig DataTableauTeradataPythonSQLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'Java', 'Pyspark', 'Hive', 'GitHub', 'Hadoop', 'Big Data', 'Tableau', 'Teradata', 'Python', 'SQL']","['UG :BCA in Computers, B.Sc in Computers, B.Tech/B.E. in Computers', 'PG :Any Postgraduate']","Industry TypeEvents / Live Entertainment,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleData Engineer,",Not Disclosed,3 - 5 years,Bangalore/Bengaluru,"Job description- 2+ years of development experience in at least one of MySQL, Oracle, PostgreSQL or MSSQL and with Big Data frameworks / platforms / data stores like Apache Drill, Arrow, Hadoop, HDFS, Spark, MapR etc- Strong experience setting up data warehouses, data modeling, data wrangling and dataflow architecture on the cloud- Strong experience in 3+ experience with public cloud services such as AWS, Azure, or GCP and languages like Java/ Python etc- 2+ years of development experience in Amazon Redshift, Google Bigquery or Azure data warehouse platforms preferred- Knowledge of statistical analysis tools like R, SAS etc- Familiarity with any data visualization software- A growth mindset and passionate about building things from the ground up and most importantly, you should be fun to work withAs a data engineer at, you will:- Create and maintain optimal data pipeline architecture,- Assemble large, complex data sets that meet functional / non-functional business requirements.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS - big data- technologies.- Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.- Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.- Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.- Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.RoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Sc in Any Specialization, B.Tech/B.E. in Any Specialization, BCA in Any SpecializationPG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsData EngineeringMapRdata modelingPostgreSQLMySQLHadoopBig DataHDFSSparkSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'MapR', 'data modeling', 'PostgreSQL', 'MySQL', 'Hadoop', 'Big Data', 'HDFS', 'Spark']","['UG :B.Sc in Any Specialization, B.Tech/B.E. in Any Specialization, BCA in Any Specialization', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Leading Client,"RoleDatabase Architect / Designer,",Not Disclosed,5 - 10 years,Bangalore/Bengaluru,"Job descriptionR- BIG QUERY+ BIG DATA/ANALYTICS/PYSPARKJob Responsibilities :- Use different data warehousing concepts to build a data warehouse for reporting purpose- Assist Analytics/ Reporting teams in setting up data pipelines & monitoring daily jobs- Lead and provide guidance to junior members in team. Oversee project life cycle from intake through delivery- Responsible for Planning, Architecture and Design for journey towards Spark, Hadoop Big Data and cloud solutions like AWS, Databricks, etc- Develop and test ETL components to high standards of data quality and act as hands-on development leadMust have :- 4+ years of experience in ETL (or) data engineering role in an analytics environment- Bachelor's degree from Tier I colleges (Comp. Sci degree preferred not mandatory)- Expertise in building data pipelines in Big data platforms; Good understanding of Data warehousing concepts- Knowledge of Shell scripting, SQL, Python & some of the standard data science packages (Pandas, Numpy, etc.)- Prior Exposure- to Big Data Technologies (Spark, Hadoop, Hive & so on)- Exposure to ETL software like Talend, Informatica, etc. is a plus; Not Mandatory- Strong verbal and business communication skills- Strong business acumen & demonstrated an aptitude for analytics that incite actionRoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :BCA in Computers, B.Tech/B.E. in Computers, B.Sc in ComputersPG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsData SciencePysparkHiveBig Data AnalyticsData EngineeringHadoopSparkNumpyPythonSQLSkills highlighted with ‘‘ are preferred keyskills","['Data Science', 'Pyspark', 'Hive', 'Big Data Analytics', 'Data Engineering', 'Hadoop', 'Spark', 'Numpy', 'Python', 'SQL']","['UG :BCA in Computers, B.Tech/B.E. in Computers, B.Sc in Computers', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
Leading Client,"RoleDatabase Architect / Designer,",Not Disclosed,8 - 12 years,Hyderabad/Secunderabad,"Job descriptionRole and responsibilities :- Creating Project Technical Documentation- Designing Solution architecture, and work on Data Ingestion, Preparation and Transformation. Debugging the production failures and identifying the solution.- Developing efficient frameworks for development and testing using (AWS Dynamo DB, EKS, Kafka, KinesisSparkStreamingPython) to enable seamless data ingestion process on to the Hadoop platform.- Enabling Data Governance and Data Discovery on Hadoop Platform- Building data processing framework using Spark, HQL- Exposure of Security Framework with Kerberos, Ranger, Atlas- Exposure of Data Pipeline Automation using DevOps tools- Exposure of Job Monitoring framework along validations automation- Exposure of handling structured, Un Structured and Streaming dataTechnical skills requirements :- The candidate must demonstrate proficiency in,- Solid hands-on and Solution Architecting experience in Big-Data Technologies (AWS preferred)- Hands on experience in: AWS Dynamo DB, EKS, Kafka, Kinesis, Glue PySpark, EMR PySpark- Hands-on experience of programming language like Python, Scala with Spark.- Good command and working experience on HadoopMap Reduce, HDFS, Hive, HBase, and No-SQL Databases- Hands on working experience on any of the data engineeringanalytics platform (HortonworksCloudera MapR AWS), AWS preferred- Hands-on experience on Data Ingestion Apache Nifi, Apache Airflow, Sqoop, and Oozie- Hands on working experience of data processing at scale with event driven systems, message queues (Kafka FlinkSpark Streaming)- Hands on working Experience with AWS Services like EMR, Kinesis, S3, CloudFormation, Glue, API Gateway, Lake Foundation- Hands on working Experience with AWS Athena- Data Warehouse exposure on Apache Nifi, Apache Airflow, Kylo- Operationalization of ML models on AWS (e.g. deployment, scheduling, model monitoring etc.)- Feature EngineeringData Processing to be used for Model development- Experience gathering and processing raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.)- Experience building data pipelines for structuredunstructured, real-timebatch, eventssynchronous asynchronous using MQ, Kafka, Steam processing- Hands-on working experience in analyzing source system data and data flows, working with structured and unstructured data- Must be very strong in writing SQL queries- Strengthen the Data engineering team with Big Data solutions- Strong technical, analytical, and problem-solving skills- Strong organizational skills, with the ability to work autonomously as well as in a team-based environment- Pleasant Personality, Strong Communication & Interpersonal SkillsRoleDatabase Architect / Designer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategoryDBA / Data warehousingEducationUG :Any GraduatePG :Any PostgraduateDoctorate :Doctorate Not RequiredKey SkillsKafkaData AnalyticsData IngestionMQData PipelineCloudFormationBig DataAWSData GovernanceSolution ArchitectWeb ScrapingSkills highlighted with ‘‘ are preferred keyskills","['Kafka', 'Data Analytics', 'Data Ingestion', 'MQ', 'Data Pipeline', 'CloudFormation', 'Big Data', 'AWS', 'Data Governance', 'Solution Architect', 'Web Scraping']","['UG :Any Graduate', 'PG :Any Postgraduate', 'Doctorate :Doctorate Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategoryDBA / Data warehousing
"IBM has been present in India since 1992. IBM India's solutions and services span all major industries including financial services, healthcare, government, automotive, telecommunications and education, among others. As a trusted partner with wide-ranging service capabilities, IBM helps clients transform and succeed in challenging circumstances. IBM has been expanding its footprint in India - and has a presence in over 200 cities and towns across the country - either directly or through its strong business partner network. IBM India has clearly established itself as one of the leaders in the Indian Information Technology (IT) Industry - and continues to transform itself to align with global markets and geographies to grow this leadership position. Widely recognised as an employer of choice, IBM holds numerous awards for its industry-leading employment practices and policies. The diversity and breadth of the entire IBM portfolio of research, consulting, solutions, services, systems and software, uniquely distinguishes IBM India from other companies in the industry. To know more about business units at IBM India, click on the “About Us” link above.","RoleFull Stack Developer,",Not Disclosed,4 - 8 years,Bangalore/Bengaluru,"Job descriptionAs Senior Talend Developer, you will serve as a liaison among business partners, technical resources, and project stake holders to identify, articulate and facilitate business process and systems changes related to document digitization and document- driven business processes.      Your Role and Responsibilities           As Data engineer, you will develop and move data from the operational and external environments to the business intelligence environment using Ab Initio software. Skills include designing and developing extract, transform and load (ETL) processes.                Responsibilities:              Coordinate with multiple technical teams to ensure apt integration of functions to identify and define necessary system enhancements to deploy new products and process improvements           Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint           Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation           Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals           Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards           Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions,         Required Technical and Professional Expertise           Minimum 4 years of experience in ETL Datastage development           Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting           Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers         Preferred Technical and Professional Expertise           You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies           Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work           Intuitive individual with an ability to manage change and proven time management           Proven interpersonal skills while contributing to team effort by accomplishing related results as needed           Up-to-date technical knowledge by attending educational workshops, reviewing publications      RoleFull Stack Developer,Industry TypeIT Services & Consulting,Functional AreaEngineering - Software & QA,Employment TypeFull Time, PermanentRole CategorySoftware DevelopmentEducationUG :B.Tech/B.E.PG :Post Graduation Not RequiredKey SkillsBusiness processCodingDatastageBusiness intelligenceUnix shell scriptingdata cleansingInterpersonal skillsTime managementCreative designingDebuggingSkills highlighted with ‘‘ are preferred keyskills","['Business process', 'Coding', 'Datastage', 'Business intelligence', 'Unix shell scripting', 'data cleansing', 'Interpersonal skills', 'Time management', 'Creative designing', 'Debugging']","['UG :B.Tech/B.E.', 'PG :Post Graduation Not Required']","Industry TypeIT Services & Consulting,","Functional AreaEngineering - Software & QA,","Employment TypeFull Time, Permanent",Role CategorySoftware Development
Roppen Transportation Services Private Limited,"RoleData Engineer,",Not Disclosed,5 - 7 years,Bangalore/Bengaluru,"Job descriptionRole and Responsibilities:Creating complex data processing pipelines, as part of diverse, high energy teamsDesigning scalable implementations of the models developed by our Data ScientistsBeing able to deploy models in real-time applications either as part of a microservice(HTTP or RPC) with bounded context or as realtime pipelines producing events in response to user actions on groundHands-on programming based on TDD, usually in a pair programming environmentDeploying data pipelines in production based on Continuous Delivery practices.Able to build and operate Data Pipelines, Build and operate Data Storage, Is familiar with Infrastructure definition and automation in this context. Is aware of adjacent technologies to the ones they have worked on. Good understanding of Data Modelling.Involve in building and deploying large scale data processing pipelines in a production environment.Experience building data pipelines and data centric applications using distributed storage solutions(including and not limited to HDFS like storage, Elasticsearch, Mongo, Kafka, Postgres/Mysql etc)Job RequirementTechnical Competencies:Experience in HDFS, S3, NoSql Databases and distributed platforms like Hadoop, Spark, Flink, Hive, Kafka, Oozie, Airflow, Elasticsearch etc.Experience in any of MapR, Cloudera, and HortonWorks and/ or cloud based Hadoop Distributions(GCP preferred).Experience creating and building data centric application involving ML modelsFunctional / Behavioural Competencies:Actively seeking to learn newer tech and curiously experimenting is a trait that would be preferred.Excellent understanding of technology landscapeLearning ability: Applies theoretical knowledge to practiceFocus on excellenceMentoring team matesEducation & Experiences:B. Tech, M. Tech (in Computer Sciences preferred)Around 3+ years of experience. For transitioned data engineers over all experience of 5+ years in preferred.Interview Process:Round 1 – AssignmentRound 2 – Technical Discussion 1Round 3 – Technical Discussion 2/ Managerial RoundRound 4 – HR RoundRoleData Engineer,Industry TypeCourier / Logistics,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in ComputersPG :M.Tech in ComputersKey SkillsData Engineeringhiveclouderasparkoozieairflowkafkadata processinghdfshadoopMLSkills highlighted with ‘‘ are preferred keyskills","['Data Engineering', 'hive', 'cloudera', 'spark', 'oozie', 'airflow', 'kafka', 'data processing', 'hdfs', 'hadoop', 'ML']","['UG :B.Tech/B.E. in Computers', 'PG :M.Tech in Computers']","Industry TypeCourier / Logistics,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
" ""We are a set of technocratic people who believe that the data is in our DNA and impacts every little decision we make in the modern world today. The biggest first and foremost impact of data is how we define money and consumption of money across the financial sector. With this conviction, we are currently focused vertically on the BFSI & Fintech domain. Horizontally, we are working on the most cutting-edge data technology ecosystem and helping our clients build products for the future of how the whole human lives consume disruptive technology and its benefits. In the pursuit of building this inevitable, we are always on the lookout to hire absolutely great next-gen talent who is not afraid of taking risks to make themselves relevant for the future. As an organization we have created a platform for young and dynamic engineering professionals to make mistakes, explore new technologies, and make themselves better with each passing day, while  working on some of the most complex yet thrilling business problem statements, to solve real-world problems."" ","RoleData Engineer,",Not Disclosed,2 - 6 years,Pune,"Job description Position: Data EngineerPackage: Best in class(Depending on the skillsets, experience & fitment)_______________________________________________________________________________________________People who are serving their notice period or have served their notice period and who can demonstrate joining date in the next***** 10 to 12 *****days are only expected to apply for this job_______________________________________________________________________________________________Core Responsibilities : The candidate is expected to lead one of the key analytics area end to end. This is a pure hands on role. Ensure the solutions built meet the required best practices and coding standards. Ability to adapt any new technology if situation demands. Requirement gathering with business and get this prioritized in the sprint cycle. Should be able to take end to end responsibility of assigned task Ensure quality and timely delivery.Preference and Experience: Strong at PySpark, Python, Java fundamentals Good understanding of Data Structure Good at SQL query/optimization Strong fundamental of OOPs programming Good understanding of AWS Cloud,Big Data. Nice to have Data Lake,AWS Glue, Athena, S3, Kinesis, SQL/NoSQL DBAcademic qualifications : Must be a Technical Graduate  Btech / Mtech  Tier 1/2 colleges.Experience Range: 2 to 6 yearRoleData Engineer,Industry TypeIT Services & Consulting,Functional AreaData Science & Analytics,Employment TypeFull Time, PermanentRole CategoryData Science & Machine LearningEducationUG :B.Tech/B.E. in Any SpecializationPG :M.Tech in Any SpecializationKey SkillsPysparkData MigrationData PipelineAWSSQLData StructuresData AnalysisSparkETLPythonData IntegrationSkills highlighted with ‘‘ are preferred keyskills","['Pyspark', 'Data Migration', 'Data Pipeline', 'AWS', 'SQL', 'Data Structures', 'Data Analysis', 'Spark', 'ETL', 'Python', 'Data Integration']","['UG :B.Tech/B.E. in Any Specialization', 'PG :M.Tech in Any Specialization']","Industry TypeIT Services & Consulting,","Functional AreaData Science & Analytics,","Employment TypeFull Time, Permanent",Role CategoryData Science & Machine Learning
